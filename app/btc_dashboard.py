import streamlit as st
import pandas as pd
import os
import joblib 
import plotly.graph_objects as go
import numpy as np
import plotly.express as px
from datetime import datetime
import statsmodels.api as sm
from statsmodels.formula.api import ols
from sklearn.neighbors import KernelDensity


# ==================================================================================================
# 1. STREAMLIT KONFIGURATION
# ==================================================================================================
st.set_page_config(layout="wide", page_title="‚Çø BTC Markt-Regime & Prognose-Dashboard") 

# ==================================================================================================
# 2. DATENPFADE & LADEFUNKTIONEN (FINAL KORRIGIERT: Trennzeichen-Konflikt gel√∂st)
# ==================================================================================================

# üõ†Ô∏è ABSOLUTE PFADE (Basierend auf Ihren Angaben)
DATA_PATH = '/Users/burcukiran/Desktop/Abschlussprojekt_Data_Science/data/processed/btc_master_view_final.csv'
PCA_DATA_PATH = '/Users/burcukiran/Desktop/Abschlussprojekt_Data_Science/data/processed/btc_clusters_pca.csv'
REGRESSION_METRICS_PATH = '/Users/burcukiran/Desktop/Abschlussprojekt_Data_Science/data/processed/price_daily_model_metrics.csv'
CLUSTERING_METRICS_PATH = '/Users/burcukiran/Desktop/Abschlussprojekt_Data_Science/data/processed/clustering_metrics.csv'
DIRECTION_METRICS_PATH = '/Users/burcukiran/Desktop/Abschlussprojekt_Data_Science/models/direction/direction_model_metrics.csv' # üö® L√§d mit sep=','

# --- Ladefunktion f√ºr die Master-Datei (nutzt Semikolon) ---
@st.cache_data
def load_data(path):
    """L√§dt die Master-Datei und f√ºhrt erste Bereinigungen durch."""
    st.info(f"üìÅ Versuche, Master-Datei von Pfad: **{path}** zu laden...")
    
    if not os.path.exists(path):
        st.error(f"‚ùå **Datei nicht gefunden!** Der Pfad existiert nicht: `{path}`")
        return None
    try:
        # ‚úî Die Master-Datei verwendet ein Semikolon (;)
        df = pd.read_csv(path, sep=';', index_col='Date', parse_dates=True, on_bad_lines='skip') 
        
        required_cols = ['PC1', 'PC2', 'Regime', 'Close']
        if not all(col in df.columns for col in required_cols):
             st.error(f"‚ùå **Fehlende Spalten im Master-DF.** Erwartet: {required_cols}")
             return None
        
        # Erzwingen der numerischen Konvertierung (Beh√§lt vorhandenen Fix)
        df['PC1'] = pd.to_numeric(df['PC1'], errors='coerce')
        df['PC2'] = pd.to_numeric(df['PC2'], errors='coerce')
        df.dropna(subset=['PC1', 'PC2', 'Regime'], inplace=True) 
        
        st.success(f"‚úÖ **Daten erfolgreich geladen und PCA-Spalten als Float konvertiert!** Shape: {df.shape}")
        return df
    except Exception as e:
        st.error(f"‚ö†Ô∏è **Schwerer Fehler beim Einlesen der CSV**: `{e}`")
        return None

# --- Ladefunktion f√ºr Metriken, die wir KOMMAGETRENNT gespeichert haben (FIX F√úR DIRECTION_METRICS) ---
@st.cache_data
def load_metrics_comma_sep(path):
    """
    L√§dt die Klassifikations-Metriken explizit mit Komma-Separator.
    Dies behebt den Fehler beim Parsen von direction_model_metrics.csv.
    """
    if not os.path.exists(path):
        st.warning(f"‚ö†Ô∏è WARNUNG: Metrik-Datei nicht gefunden unter `{path}`.")
        return None
    try:
        # ‚úî WICHTIG: Erzwingt das Komma-Trennzeichen, das im Jupyter Notebook genutzt wird.
        df = pd.read_csv(path, sep=',')
        return df
    except Exception as e:
        st.error(f"‚ùå Fehler beim Laden der Metriken unter {path}: {e}")
        return None

# --- Urspr√ºngliche, robuste Ladefunktion (Wird beibehalten, da sie f√ºr andere Files funktioniert) ---
@st.cache_data
def load_metrics_robust(path, expected_cols=None):
    """
    L√§dt eine Metrik-Datei und versucht es mit verschiedenen Separatoren, 
    um Parsing-Fehler zu vermeiden (wird f√ºr Regress./Cluster-Files genutzt).
    """
    if not os.path.exists(path):
        st.warning(f"‚ö†Ô∏è WARNUNG: Metrik-Datei nicht gefunden unter `{path}`.")
        return None
    
    # üí° Wir lassen die urspr√ºngliche Logik, um andere Metriken zu laden.
    separators = [';', ',', '\t']
    df = None
    
    for sep in separators:
        try:
            df = pd.read_csv(path, sep=sep)
            if df.shape[1] > 1:
                if expected_cols is None or any(col in df.columns for col in expected_cols):
                    return df
        except Exception:
            continue
            
    st.error(f"‚ùå Metriken konnten nicht geladen oder richtig geparst werden: `{path}`. Versuchte Separatoren: {separators}. √úberpr√ºfen Sie das Dateiformat.")
    return None

def load_direction_metrics():
    path = "/Users/burcukiran/Desktop/Abschlussprojekt_Data_Science/models/direction/direction_model_metrics.csv"
    try:
        return pd.read_csv(path)
    except Exception as e:
        print("Fehler beim Laden der Direction-Metriken:", e)
        return None
    
def create_direction_model_performance_plot():
    df = load_direction_metrics()
    if df is None:
        return go.Figure().add_annotation(
            text="Direction-Metriken konnten nicht geladen werden.",
            x=0.5, y=0.5, showarrow=False
        )

    # F1-Spalte automatisch finden
    f1_col = None
    for col in df.columns:
        if "f1" in col.lower():
            f1_col = col
            break

    if f1_col is None:
        return go.Figure().add_annotation(
            text=f"Keine F1-Spalte gefunden. Spalten: {list(df.columns)}",
            x=0.5, y=0.5, showarrow=False
        )

    df_grouped = df.groupby("Modell")[f1_col].mean().reset_index()
    df_grouped["f1_pct"] = df_grouped[f1_col] * 100

    fig = px.bar(
        df_grouped,
        x="Modell",
        y="f1_pct",
        color="Modell",
        title="Pr√ºfung These 7: Modellgenauigkeit (F1-Score) f√ºr Up/Down-Vorhersage",
        labels={"f1_pct": "F1-Score (%)"},
        text=df_grouped["f1_pct"].round(1).astype(str) + "%",
        template="plotly_white"
    )

    fig.update_yaxes(range=[40, 80])
    fig.update_traces(textposition="outside")

    return fig
# ==================================================================================================
# 3. DATENLADUNG STARTEN
# ==================================================================================================

df_master = load_data(DATA_PATH)


# --- KORREKTUR F√úR CLUSTER_3 DATENTYP (HIER EINF√úGEN) ---
# Konvertiert 0.0, 1.0, 2.0 zu den Strings '0', '1', '2' f√ºr Plotly

df_master['Cluster_3'] = df_master['Cluster_3'].fillna(-1).astype(int).astype(str)
# --- KLASSISCHE DATENLADUNG ---
# Nutzt die robuste Funktion f√ºr die Files, bei denen wir den Separator nicht kennen
df_cluster_metrics = load_metrics_robust(CLUSTERING_METRICS_PATH, expected_cols=['model', 'Model', 'Silhouette'])
df_reg_metrics = load_metrics_robust(REGRESSION_METRICS_PATH, expected_cols=['model', 'Model', 'horizon', 'Horizont'])


# --- üö® KORRIGIERTE DATENLADUNG F√úR KLASSIFIKATIONS-METRIKEN ---
# Nutzt die neue Funktion, um den Komma-Separator zu erzwingen
df_dir_metrics = load_metrics_comma_sep(DIRECTION_METRICS_PATH)


# --- üö® KRITISCHE KORREKTUR DER REGRESSIONS-METRIKEN ---
# Behebt den 'horizon'-Fehler und den 'Modell'-KeyError durch Standardisierung der Spaltennamen
if df_reg_metrics is not None:
    
    # Standardisiere den Modell-Namen auf 'model'
    model_col = next((col for col in ['model', 'Model'] if col in df_reg_metrics.columns), None)
    if model_col is not None:
        df_reg_metrics.rename(columns={model_col: 'model'}, inplace=True)
    else:
        st.error("‚ùå Schwerer Fehler: Die Spalte 'model' (Modellname) fehlt in den Regressions-Metriken.")
        df_reg_metrics = None # Deaktiviere den Tab bei fehlender Schl√ºsselspalte

    # Standardisiere den Horizont-Namen auf 'horizon' und konvertiere
    horizon_col = next((col for col in ['horizon', 'Horizont'] if col in df_reg_metrics.columns), None)
    
    if df_reg_metrics is not None and horizon_col is not None:
        try:
            # Versuche, 7 (int) zu "7d" (string) zu konvertieren
            df_reg_metrics['horizon'] = df_reg_metrics[horizon_col].astype(str).str.replace('d', '')
            df_reg_metrics['horizon'] = df_reg_metrics['horizon'].astype(int).astype(str) + 'd'
            
            # Bereinigung der Spalten (wenn n√∂tig)
            if horizon_col != 'horizon':
                 df_reg_metrics.drop(columns=[horizon_col], inplace=True)
                 
            st.success("‚úÖ Regressions-Metriken: 'horizon' Spalte erfolgreich f√ºr Filterung konvertiert.")
        except Exception as e:
            st.error(f"‚ö†Ô∏è Fehler bei der Konvertierung der 'horizon'-Spalte in df_reg_metrics: {e}. Pr√ºfen Sie die Werte.")
            df_reg_metrics = None # Deaktiviere den Tab bei fehlerhafter Konvertierung
    elif df_reg_metrics is not None:
        st.error("‚ùå Schwerer Fehler: Die Spalte 'horizon' fehlt in den geladenen Regressions-Metriken. (Wird ignoriert)")
        df_reg_metrics = None
# -------------------------------------------------------------------------


# ==================================================================================================
# 3. HILFSFUNKTIONEN F√úR DATAFRAME STYLING UND FORMATIERUNG
# ==================================================================================================

# --- STYLING FUNKTIONEN (Jetzt abgesichert mit .get() gegen KeyError) ---

def highlight_selected_reg_model(s, current_model):
    """Hebt das aktuell ausgew√§hlte Regressionsmodell hervor (Spalte 'Modell')."""
    # Fix: Verwende .get() um KeyError zu vermeiden, falls die Spalte fehlt
    return ['background-color: #60A5FA' if s.get('Modell') == current_model else '' for _ in s]

def highlight_selected_dir_model(s, current_model):
    """Hebt das aktuell ausgew√§hlte Klassifikationsmodell hervor (Spalte 'Modell')."""
    # Fix: Verwende .get() um KeyError zu vermeiden, falls die Spalte fehlt
    return ['background-color: #60A5FA' if s.get('Modell') == current_model else '' for _ in s]

def highlight_selected_cluster_model(s, current_model):
    """Hebt das aktuell ausgew√§hlte Clustering-Modell hervor (Spalte 'Modell')."""
    return ['background-color: #60A5FA' if s.get('Modell') == current_model else '' for _ in s]

def highlight_focus_day(s, focus_date_str):
    """Hebt den Fokus-Tag in der Zeile hervor (angewendet auf axis=1)."""
    try:
        current_date_str = s.name.strftime('%Y-%m-%d')
    except AttributeError:
        current_date_str = str(s.name)

    if current_date_str == focus_date_str:
        return ['background-color: #FBBF24; color: black; font-weight: bold;'] * len(s)
    
    return [''] * len(s)

def highlight_return_analogy(s):
    """Hebt positive/negative Returns in der Analogie-Tabelle hervor."""
    # Der Style muss auf die Zelle in der Spalte 'Tats√§chlicher 7d Return (Real)' angewendet werden.
    if s.name == 'Tats√§chlicher 7d Return (Real)':
        styles = []
        for val_str in s:
            try:
                # Versuche, den Wert als Zahl zu interpretieren (entferne "%" und Leerzeichen)
                val = float(str(val_str).replace('%', '').strip().replace(',', '.'))
                
                if val > 0:
                    styles.append('background-color: rgba(40, 167, 69, 0.4); color: black')
                elif val < 0:
                    styles.append('background-color: rgba(220, 53, 69, 0.4); color: white')
                else:
                    styles.append('')
            except ValueError:
                styles.append('')
        return styles
    # R√ºckgabe einer leeren Liste von Styles, wenn es nicht die Zielspalte ist
    return [''] * len(s)
    


# --- FORMATIERUNGS FUNKTIONEN (F√ºr saubere Anzeige) ---

def format_prices(df):
    """Formatiert W√§hrungswerte und Kennzahlen f√ºr die Anzeige."""
    df_f = df.copy()
    price_cols = [col for col in ['Open', 'High', 'Low', 'Close', 'MA7', 'MA21', 'MA50', 'MA200'] if col in df_f.columns]
    for col in price_cols:
        df_f[col] = pd.to_numeric(df_f[col], errors='coerce').apply(lambda x: f'{x:,.2f}') 
    
    ratio_cols = [col for col in ['Return', 'Volatility30', 'Momentum7'] if col in df_f.columns]
    for col in ratio_cols:
        df_f[col] = pd.to_numeric(df_f[col], errors='coerce').apply(lambda x: f'{x:.4f}')
        
    if 'Volume' in df_f.columns:
        df_f['Volume'] = pd.to_numeric(df_f['Volume'], errors='coerce').apply(lambda x: f'{x:,.0f}')
        
    return df_f

def format_cluster_pca(df):
    """Formatiert PCA-Werte und benennt Cluster-ID um."""
    df_c = df.copy()
    if 'Cluster_3' in df_c.columns:
        df_c.rename(columns={'Cluster_3': 'Cluster_ID'}, inplace=True)
        
    pca_cols = [col for col in ['PC1', 'PC2'] if col in df_c.columns]
    for col in pca_cols:
        df_c[col] = pd.to_numeric(df_c[col], errors='coerce').apply(lambda x: f'{x:.3f}')
        
    return df_c

def format_signals(df):
    """Formatiert Wahrscheinlichkeiten und Preisprognosen."""
    df_s = df.copy()
    
    if 'Prob_Up' in df_s.columns:
        df_s['Prob_Up'] = pd.to_numeric(df_s['Prob_Up'], errors='coerce').apply(lambda x: f'{x:.3f}')
        
    pred_cols = [col for col in ['Pred_1d', 'Pred_7d', 'Pred_30d', 'Pred_90d', 'Pred_365d'] if col in df_s.columns]
    for col in pred_cols:
        df_s[col] = pd.to_numeric(df_s[col], errors='coerce').apply(lambda x: f'{x:,.2f}')
        
    return df_s

# ==================================================================================================
# 4. PLOTLY CHART & ANALOGIE FUNKTIONEN
# ==================================================================================================

def create_candlestick_chart(df):
    """Erstellt einen Plotly Candlestick Chart mit MAs und Regime-Hintergrund."""
    
    REGIME_COLORS = {
        'Bull': 'rgba(40, 167, 69, 0.15)',      
        'Bear': 'rgba(220, 53, 69, 0.15)',      
        'Sideways': 'rgba(255, 193, 7, 0.15)'  
    }
    
    fig = go.Figure()
    shapes = []
    
    # Sicherstellen, dass die Daten f√ºr min/max numerisch sind
    df_numeric = df[['Low', 'High', 'Close']].apply(pd.to_numeric, errors='coerce')
    
    try:
        y_min = df_numeric['Low'].min() * 0.99
        y_max = df_numeric['High'].max() * 1.01
    except:
        y_min = df_numeric['Close'].min() * 0.95
        y_max = df_numeric['Close'].max() * 1.05


    # F√ºge Rechtecke f√ºr jeden Tag mit dem entsprechenden Regime hinzu
    for i in range(len(df)):
        date_start = df.index[i]
        
        if i < len(df) - 1:
            date_end = df.index[i+1]
        else:
            date_end = date_start + pd.Timedelta(days=1) 
            
        regime = df['Regime'].iloc[i]
        color = REGIME_COLORS.get(regime, 'rgba(108, 117, 125, 0.1)') 
        
        shapes.append(
            dict(
                type="rect",
                x0=date_start,
                x1=date_end,
                y0=y_min,
                y1=y_max,
                fillcolor=color,
                layer="below",
                line_width=0,
            )
        )
        
    fig.update_layout(shapes=shapes)

    # Candlestick Trace
    fig.add_trace(go.Candlestick(
        x=df.index,
        open=df['Open'],
        high=df['High'],
        low=df['Low'],
        close=df['Close'],
        name='BTC/USD Preis',
        increasing_line_color='#28a745', 
        decreasing_line_color='#dc3545'  
    ))

    # Moving Average Traces
    ma_colors = {
        'MA7': 'blue', 
        'MA21': 'purple', 
        'MA50': 'orange', 
        'MA200': 'red'
    }
    
    for ma, color in ma_colors.items():
        if ma in df.columns:
            df[ma] = pd.to_numeric(df[ma], errors='coerce')
            fig.add_trace(go.Scatter(
                x=df.index, 
                y=df[ma], 
                mode='lines', 
                name=ma, 
                line=dict(color=color, width=1.5)
            ))

    # Layout-Anpassungen
    fig.update_layout(
        title=f'BTC/USD Kursverlauf, MAs & Markt-Regime ({df.index.min().strftime("%Y-%m-%d")} bis {df.index.max().strftime("%Y-%m-%d")})',
        xaxis_title='Datum',
        yaxis_title='Preis (USD)',
        xaxis_rangeslider_visible=False, 
        legend=dict(
            orientation="h",
            yanchor="bottom",
            y=1.02,
            xanchor="right",
            x=1
        ),
        height=600,
        margin=dict(l=20, r=20, t=60, b=20),
        template="plotly_white"
    )

    fig.update_xaxes(type='date', rangeselector=None) 
    
    return fig

def create_pca_scatter_plot(df, focus_date_str):
    """Erstellt einen interaktiven Scatter Plot der PCA-Komponenten (PC1 vs. PC2),
    eingef√§rbt nach dem Markt-Regime, und hebt den Fokus-Tag hervor. (Fix f√ºr den PCA Plot)"""
    
    # Sicherstellen, dass PC1 und PC2 numerisch sind
    df['PC1'] = pd.to_numeric(df['PC1'], errors='coerce')
    df['PC2'] = pd.to_numeric(df['PC2'], errors='coerce')
    
    df_plot = df.copy().dropna(subset=['PC1', 'PC2', 'Regime'])

    REGIME_COLOR_MAP = {
        'Bear': '#DC3545',      # Rot
        'Sideways': '#FFC107',  # Gelb
        'Bull': '#28A745',      # Gr√ºn
    }
    
    fig = go.Figure()

    # 1. Historische Punkte nach Regime
    for regime, color in REGIME_COLOR_MAP.items():
        df_regime = df_plot[df_plot['Regime'] == regime]
        
        # Base historical points
        fig.add_trace(go.Scatter(
            x=df_regime['PC1'],
            y=df_regime['PC2'],
            mode='markers',
            marker=dict(
                size=5,
                color=color,
                opacity=0.6,
                line=dict(width=0.5, color='DarkSlateGrey')
            ),
            name=f'Regime: {regime}',
            text=df_regime.index.strftime('%Y-%m-%d') + '<br>Regime: ' + df_regime['Regime'],
            hoverinfo='text+x+y'
        ))

    # 2. Fokus-Tag hervorheben
    focus_date_dt = pd.to_datetime(focus_date_str).strftime('%Y-%m-%d')
    focus_data_match = df_plot[df_plot.index.strftime('%Y-%m-%d') == focus_date_dt]
    
    if not focus_data_match.empty:
        focus_data = focus_data_match.iloc[0]
        
        fig.add_trace(go.Scatter(
            x=[focus_data['PC1']],
            y=[focus_data['PC2']],
            mode='markers+text',
            marker=dict(
                size=20,
                color='#FBBF24', # Fokus-Farbe (Amber)
                line=dict(width=3, color='black'),
                symbol='star' # Stern-Symbol f√ºr den Fokus
            ),
            name='Fokus-Tag',
            text=[f'üéØ {focus_date_str} ({focus_data["Regime"]})'],
            textposition="top center",
            # HIER: Farbe auf WEISS und Gr√∂√üe auf 16 erh√∂ht (Lesbarkeit bei dunklem Hintergrund)
            textfont=dict(size=16, color='white', weight='bold'), 
            hoverinfo='text'
        ))


    # 3. Layout-Anpassungen
    fig.update_layout(
        title='Markt-Regime-Visualisierung (PC1 vs. PC2)',
        xaxis_title='PC1 (Trend-Achse: Bear ‚¨ÖÔ∏è ‚û°Ô∏è Bull)',
        yaxis_title='PC2 (Volatilit√§ts-Achse)',
        legend_title="Markt-Regime",
        height=650,
        template="plotly_white"
    )
    
    # 4. Optional: Markierung des PC1-Durchschnitts zur Trennung von Bull/Bear
    fig.add_vline(x=df_plot['PC1'].mean(), line_width=1, line_dash="dash", line_color="gray", 
                  annotation_text="Historischer Durchschnitt PC1", annotation_position="bottom right")

    return fig



def find_analogies(df_master, focus_date, top_k):
    """
    Findet die Top-K √§hnlichsten Tage basierend auf dem euklidischen Abstand
    der PCA-Komponenten (PC1, PC2).
    """
    if focus_date not in df_master.index:
        return pd.DataFrame(), None
    
    # 1. Definiere den Fokuspunkt (PCA-Werte)
    # Sicherstellen, dass die Werte als Float interpretiert werden
    df_master['PC1'] = pd.to_numeric(df_master['PC1'], errors='coerce')
    df_master['PC2'] = pd.to_numeric(df_master['PC2'], errors='coerce')
    
    focus_point = df_master.loc[focus_date, ['PC1', 'PC2']].values.astype(np.float64)
    
    # 2. Filtere den Fokus-Tag aus der Historie
    df_history = df_master.drop(focus_date, errors='ignore').copy()
    
    # 3. Berechne den euklidischen Abstand
    history_points = df_history[['PC1', 'PC2']].values.astype(np.float64)
    
    # Berechnung des euklidischen Abstands
    distances = np.linalg.norm(history_points - focus_point, axis=1)
    df_history['Distance'] = distances
    
    # 4. Sortiere und w√§hle die Top-K Tage aus
    df_analogies = df_history.sort_values(by='Distance', ascending=True).head(top_k)
    
    # 5. Berechne die Outcomes f√ºr die Analogien (7-Tage-Return)
    close_series = df_master['Close'].apply(pd.to_numeric, errors='coerce')
    real_returns = []
    
    for index in df_analogies.index:
        try:
            loc_index = df_master.index.get_loc(index)
            future_index = loc_index + 7 # 7 Tage sp√§ter
            
            if future_index < len(df_master):
                start_price = close_series.loc[index]
                # Verwende .iloc, um den Preis am zuk√ºnftigen Index zu erhalten
                end_price = close_series.iloc[future_index] 
                real_return = (end_price / start_price) - 1
            else:
                real_return = np.nan 
        except:
             real_return = np.nan
        
        real_returns.append(real_return)
    
    df_analogies['Real_Return_7d'] = real_returns
    
    # 6. Berechne die zusammenfassenden Metriken
    df_analogies_clean = df_analogies.dropna(subset=['Real_Return_7d'])
    up_count = (df_analogies_clean['Real_Return_7d'] > 0).sum()
    total_count = df_analogies_clean['Real_Return_7d'].count()
    
    summary_metrics = {
        'Tage analysiert': total_count,
        'Durchschn. 7d Return (Real)': df_analogies_clean['Real_Return_7d'].mean() * 100, # In %
        'Anteil Up (7d)': (up_count / total_count) * 100 if total_count > 0 else 0, # In %
        'Median 7d Return (Real)': df_analogies_clean['Real_Return_7d'].median() * 100 # In %
    }
    
    return df_analogies, summary_metrics


# Globale Farbkarte f√ºr Konsistenz
REGIME_COLOR_MAP = {
    'Bear': '#DC3545',      # Rot
    'Sideways': '#FFC107',  # Gelb
    'Bull': '#28A745',      # Gr√ºn
}

# # --- 1. EDA-FUNKTIONEN (Sektion 5.1) ---

def create_return_distribution_plot(df):
    """Histogramm + Boxplot der t√§glichen Renditen (EDA)."""
    fig = px.histogram(
        df,
        x="Return",
        nbins=120,
        marginal="box",
        title="Verteilung der t√§glichen Renditen (Return)",
        template="plotly_white"
    )
    fig.update_xaxes(title="T√§gliche Rendite (%)")
    fig.update_yaxes(title="H√§ufigkeit")
    return fig

def create_correlation_heatmap(df):
    """Korrelationen der wichtigsten Features (EDA)."""
    cols = ["Return", "Volatility30", "Momentum7", "MA50", "MA200"]
    corr = df[cols].corr()

    fig = px.imshow(
        corr,
        text_auto=True,
        color_continuous_scale="RdBu_r",
        title="Korrelationsmatrix der Markt-Features",
        labels=dict(color="Korrelationswert")
    )
    fig.update_layout(height=500)
    return fig

def create_regime_frequency_bar_chart(df):
    """Erstellt ein Balkendiagramm der H√§ufigkeit der Markt-Regime."""
    
    df_counts = df['Regime'].value_counts().reset_index()
    df_counts.columns = ['Regime', 'Anzahl Tage']

    order = ['Bull', 'Sideways', 'Bear']
    df_counts['Regime'] = pd.Categorical(df_counts['Regime'], categories=order, ordered=True)
    df_counts = df_counts.sort_values('Regime')

    fig = px.bar(
        df_counts,
        x='Regime',
        y='Anzahl Tage',
        color='Regime',
        color_discrete_map=REGIME_COLOR_MAP,
        text='Anzahl Tage',
        title='H√§ufigkeit der Markt-Regime'
    )

    fig.update_traces(texttemplate='%{text}', textposition='outside')
    fig.update_layout(height=420, yaxis_title="Anzahl Tage", xaxis_title=None, showlegend=False)

    return fig



# def create_regime_frequency_bar_chart(df):
#     """Erstellt ein Balkendiagramm, das die absoluten H√§ufigkeiten der Markt-Regime zeigt (Teil 1)."""
#     df_counts = df['Regime'].value_counts().reset_index()
#     df_counts.columns = ['Regime', 'Anzahl Tage']
#     order = ['Bull', 'Sideways', 'Bear']
#     df_counts['Regime'] = pd.Categorical(df_counts['Regime'], categories=order, ordered=True)
#     df_counts = df_counts.sort_values('Regime')
    
#     fig = px.bar(df_counts, x='Regime', y='Anzahl Tage', color='Regime', color_discrete_map=REGIME_COLOR_MAP,
#                  title='H√§ufigkeit der Markt-Regime', text='Anzahl Tage')
#     fig.update_layout(xaxis_title=None, yaxis_title='Anzahl Tage', showlegend=False, height=400)
#     fig.update_traces(texttemplate='%{text}', textposition='outside')
#     return fig

# def create_cluster_frequency_bar_chart(df):
#     """Erstellt ein Balkendiagramm, das die absoluten H√§ufigkeiten der ML-Cluster zeigt (Teil 1)."""
#     df_counts = df['Cluster_3'].value_counts().reset_index()
#     df_counts.columns = ['Cluster', 'Anzahl Tage']
#     df_counts['Cluster'] = df_counts['Cluster'].astype(str)
    
#     fig = px.bar(df_counts, x='Cluster', y='Anzahl Tage', color='Cluster',
#                  title='H√§ufigkeit der ML-Cluster (K=3)', text='Anzahl Tage')
#     fig.update_layout(xaxis_title=None, yaxis_title='Anzahl Tage', showlegend=False, height=400)
#     fig.update_traces(texttemplate='%{text}', textposition='outside')
#     return fig

# def create_time_series_plot(df):
#     """Zeitlicher Verlauf von Preis und Volatilit√§t (Teil 1)."""
#     df['Date'] = pd.to_datetime(df.index)
#     df_ts = df[['Close', 'Volatility30']].copy().dropna()
    
#     fig = go.Figure()
    
#     # Preis (Close) auf prim√§rer Y-Achse
#     fig.add_trace(go.Scatter(x=df_ts.index, y=df_ts['Close'], name='Close Preis', yaxis='y1', line=dict(color='blue')))
    
#     # Volatilit√§t auf sekund√§rer Y-Achse
#     fig.add_trace(go.Scatter(x=df_ts.index, y=df_ts['Volatility30'], name='Volatility30', yaxis='y2', line=dict(color='orange', dash='dot')))
    
#     fig.update_layout(
#         title='Zeitlicher Verlauf von Preis und Volatilit√§t (30d)',
#         xaxis_title='Datum',
#         yaxis=dict(title='Schlusskurs (Close)', color='blue'),
#         yaxis2=dict(title='Volatilit√§t (30d)', overlaying='y', side='right', color='orange', showgrid=False),
#         height=500,
#         legend=dict(x=0.01, y=0.99)
#     )
#     return fig

# ========================================================
# PLOT-FUNKTIONEN F√úR 5.2 THESENPR√úFUNG
# =========================================================

# THESEN BLOCK 1: These 1 (Regime-Unterschiede via Histogramme)
def create_return_by_regime_histograms(df):
    """Erzeugt drei Histogramme der t√§glichen Renditen ‚Äì je eines f√ºr Bear-, Sideways- und Bull-Phasen (These 1).
    Ziel: Regime sollen leicht verst√§ndlich und visuell klar unterscheidbar dargestellt werden.
    """

    import plotly.subplots as sp
    import plotly.graph_objects as go

    if 'Regime' not in df.columns or 'Return' not in df.columns:
        return go.Figure().add_annotation(
            text="Fehlende Spalten (Regime/Return) f√ºr These 1.",
            xref="paper", yref="paper",
            x=0.5, y=0.5, showarrow=False
        )

    regime_order = ['Bear', 'Sideways', 'Bull']
    colors = {'Bear': '#EF553B', 'Sideways': '#FECB52', 'Bull': '#00CC96'}

    # Return numerisch setzen
    df['Return'] = pd.to_numeric(df['Return'], errors='coerce')
    df_plot = df.dropna(subset=['Regime', 'Return'])

    # Subplots erstellen: 1 Zeile, 3 Spalten
    fig = sp.make_subplots(
        rows=1, cols=3,
        subplot_titles=("Bear", "Sideways", "Bull"),
        horizontal_spacing=0.12
    )

    for i, regime in enumerate(regime_order):
        regime_data = df_plot[df_plot['Regime'] == regime]['Return']

        fig.add_trace(
            go.Histogram(
                x=regime_data,
                marker_color=colors[regime],
                name=regime,
                opacity=0.75
            ),
            row=1, col=i+1
        )

        # Achsentitel + Prozentformat
        fig.update_xaxes(title_text="Rendite (%)", tickformat=".1%", row=1, col=i+1)
        fig.update_yaxes(title_text="H√§ufigkeit", row=1, col=1)

    fig.update_layout(
        title="Pr√ºfung These 1: Histogramme der t√§glichen Renditen pro Markt-Regime",
        template="plotly_white",
        height=450,
        showlegend=False
    )

    return fig

# THESEN BLOCK 2: These 2 (Cluster-Phasen)
def create_return_by_cluster_violinplot(df):
    """Erzeugt ein Violinplot der t√§glichen Rendite gruppiert nach ML-Cluster (These 2).
    These 2: Die ML-Cluster (Cluster_3) bilden strukturell unterschiedliche Marktphasen ab,
    die sich in der Renditeverteilung widerspiegeln.
    """

    if 'Cluster_3' not in df.columns or 'Return' not in df.columns:
        return go.Figure().add_annotation(
            text="Fehlende Spalten (Cluster_3/Return) f√ºr These 2.",
            xref="paper", yref="paper", x=0.5, y=0.5, showarrow=False
        )

    # Copy
    df_plot = df.copy()

    # FIX 1: Cluster_3 sicher als numerisch casten
    df_plot['Cluster_3'] = pd.to_numeric(df_plot['Cluster_3'], errors='coerce')

    # FIX 2: Nur g√ºltige Cluster >= 0 behalten
    df_plot = df_plot[df_plot['Cluster_3'].notna() & (df_plot['Cluster_3'] >= 0)]

    # Cluster-ID in String f√ºr Kategorien
    df_plot['Cluster_3_str'] = df_plot['Cluster_3'].astype(int).astype(str)

    # Return casten
    df_plot['Return'] = pd.to_numeric(df_plot['Return'], errors='coerce')
    df_plot.dropna(subset=['Cluster_3_str', 'Return'], inplace=True)

    # Sortierte Cluster-Reihenfolge
    cluster_order = sorted(df_plot['Cluster_3_str'].unique(), key=lambda x: int(x))

    # Plot
    fig = px.violin(
        df_plot,
        x='Cluster_3_str',
        y='Return',
        category_orders={'Cluster_3_str': cluster_order},
        color='Cluster_3_str',
        title='Pr√ºfung These 2: Renditeverteilung nach ML-Cluster (Cluster_3)',
        labels={'Return': 'T√§gliche Rendite (%)', 'Cluster_3_str': 'ML-Cluster'},
        template='plotly_white',
        box=True,
        points="all"
    )

    fig.update_layout(yaxis_tickformat='.2%')
    fig.update_xaxes(title_text='ML-Cluster ID')
    fig.update_yaxes(title_text='T√§gliche Rendite')

    return fig


# THESEN BLOCK 3: These 3 (Hitrate pro Cluster)
def create_hitrate_by_cluster_bar_chart(df):
    """Erzeugt ein Balkendiagramm der Trefferquote pro Cluster (These 3).
    These 3: Das Prognosemodell arbeitet in einigen Clustern besser als in anderen.
    """

    try:
        # ‚ùó DIESER TEIL IST SIMULIERT ‚Äì sp√§ter mit echten Daten ersetzen
        hitrate_data = pd.DataFrame({
            'Cluster_3': [0, 1, 2],
            'Hit_Rate': [0.52, 0.75, 0.65],
            'Cluster_Label': ['Cluster 0', 'Cluster 1', 'Cluster 2']
        })

        hitrate_data = hitrate_data.sort_values(by='Hit_Rate', ascending=False)

        fig = px.bar(
            hitrate_data,
            x='Cluster_Label',
            y='Hit_Rate',
            color='Hit_Rate',
            color_continuous_scale=px.colors.sequential.Teal,
            title='Pr√ºfung These 3: Trefferquote nach ML-Cluster',
            labels={'Hit_Rate': 'Trefferquote', 'Cluster_Label': 'ML-Cluster'},
            template='plotly_white'
        )

        fig.update_yaxes(range=[0.4, 0.9], tickformat='.1%')
        fig.update_xaxes(title_text='ML-Cluster (sortiert)')
        fig.update_yaxes(title_text='Trefferquote (Hit Rate)')

    except Exception as e:
        fig = go.Figure()
        fig.add_annotation(text=f"Datenfehler f√ºr These 3: {e}",
                           xref="paper", yref="paper", x=0.5, y=0.5, showarrow=False)

    return fig


# # THESEN BLOCK 4: These 4 (Cluster vs. Regime Mapping)
# def create_cluster_regime_heatmap(df):
#     """Erstellt eine Heatmap der H√§ufigkeiten, die zeigt, wie Cluster_3 auf die Regime mappt (These 4).
#     These: Die ML-Cluster korrelieren stark mit den Regime-Labels, sind aber feiner granuliert.
#     """

#     # Sicherstellen, dass Spalten vorhanden sind
#     if 'Regime' not in df.columns or 'Cluster_3' not in df.columns:
#         return go.Figure().add_annotation(
#             text="Daten f√ºr These 4 fehlen (Regime oder Cluster_3 Spalte nicht gefunden).",
#             xref="paper", yref="paper", x=0.5, y=0.5, showarrow=False
#         )

#     # Copy
#     df_plot = df.copy()

#     # FIX 1: Cluster_3 numerisch machen (egal ob string, float, object‚Ä¶)
#     df_plot['Cluster_3'] = pd.to_numeric(df_plot['Cluster_3'], errors='coerce')

#     # FIX 2: Nur g√ºltige Cluster >= 0 (NaN & -1 entfernen)
#     df_plot = df_plot[df_plot['Cluster_3'].notna() & (df_plot['Cluster_3'] >= 0)]

#     # F√ºr Plot zu String casten
#     df_plot['Cluster_3'] = df_plot['Cluster_3'].astype(int).astype(str)

#     # Crosstab erstellen
#     cross_tab = pd.crosstab(df_plot['Regime'], df_plot['Cluster_3'])

#     # Regime-Reihenfolge (falls alle vorhanden sind)
#     regime_order = ['Bear', 'Sideways', 'Bull']
#     if all(r in cross_tab.index for r in regime_order):
#         cross_tab = cross_tab.reindex(regime_order, axis=0)

#     # Heatmap
#     fig = px.imshow(
#         cross_tab,
#         text_auto=True,
#         color_continuous_scale='Viridis',
#         title='Pr√ºfung These 4: Zuordnung der ML-Cluster zu Markt-Regime (Crosstab)',
#         labels={'x': 'ML Cluster-ID (Cluster_3)', 'y': 'Markt-Regime', 'color': 'Anzahl Tage'}
#     )

#     fig.update_xaxes(title="ML Cluster-ID (Cluster_3)")
#     fig.update_yaxes(title="Markt-Regime")

#     # Colorbar verbessern
#     max_count = cross_tab.values.max()
#     fig.update_layout(
#         coloraxis_colorbar=dict(
#             title="Anzahl Tage",
#             tickvals=[0, max_count],
#             ticktext=["Gering", "Hoch"],
#         ),
#         height=450
#     )

#     return fig


# THESEN BLOCK 5: These 5 (Volatilit√§t vs. Rendite)
def create_return_vs_volatility_scatter(df):
    """Erzeugt ein Scatter Plot von Rendite vs. Volatilit√§t mit Korrelationslinie (These 5).
    These: Hohe Volatilit√§t korreliert negativ mit der Rendite (Chaos f√ºhrt zu Verlust/negativer Schiefe).
    """
    if 'Volatility30' not in df.columns or 'Return' not in df.columns or 'Regime' not in df.columns:
        return go.Figure().add_annotation(text="Fehlende Spalten (Volatility30/Return/Regime) f√ºr These 5.", xref="paper", yref="paper", x=0.5, y=0.5, showarrow=False)


    regime_color_map = {'Bear': '#EF553B', 'Sideways': '#FECB52', 'Bull': '#00CC96'}

    # Sicherstellen, dass die Spalten numerisch sind und NaN entfernt werden
    df_plot = df.copy()
    df_plot['Return'] = pd.to_numeric(df_plot['Return'], errors='coerce')
    df_plot['Volatility30'] = pd.to_numeric(df_plot['Volatility30'], errors='coerce')
    df_plot.dropna(subset=['Volatility30', 'Return', 'Regime'], inplace=True)

    fig = px.scatter(
        df_plot,
        x='Volatility30',
        y='Return',
        color='Regime',
        color_discrete_map=regime_color_map,
        title='Pr√ºfung These 5: Rendite vs. Volatilit√§t (30d)',
        labels={'Return': 'T√§gliche Rendite (%)', 'Volatility30': 'Volatilit√§t (30 Tage)'},
        template='plotly_white',
        opacity=0.6,
        trendline="ols", # Ordinary Least Squares (OLS) Regression hinzuf√ºgen
        trendline_color_override='gray'
    )

    fig.update_layout(yaxis_tickformat='.2%')
    fig.update_xaxes(title_text='Volatilit√§t (30d)')
    fig.update_yaxes(title_text='T√§gliche Rendite')

    return fig

# THESEN BLOCK 6: These 6 (MA-Differenz)
def create_ma_diff_by_regime_boxplot(df):
    """Erzeugt ein Boxplot der MA50 - MA200 Differenz nach Markt-Regime (These 6).
    These: Die MA-Differenz (MA50 - MA200) ist ein valider Indikator f√ºr das Regime.
    """
    if 'MA50' not in df.columns or 'MA200' not in df.columns or 'Regime' not in df.columns:
        return go.Figure().add_annotation(text="Fehlende Spalten (MA50/MA200/Regime) f√ºr These 6.", xref="paper", yref="paper", x=0.5, y=0.5, showarrow=False)


    df_temp = df.copy()

    # Sicherstellen, dass 'MA50' und 'MA200' numerisch sind und die Differenz berechnet wird
    try:
        df_temp['MA50'] = pd.to_numeric(df_temp['MA50'], errors='coerce')
        df_temp['MA200'] = pd.to_numeric(df_temp['MA200'], errors='coerce')
        df_temp.dropna(subset=['MA50', 'MA200', 'Regime'], inplace=True)
        df_temp['MA_Diff'] = df_temp['MA50'] - df_temp['MA200']
    except KeyError as e:
        print(f"Fehler: Spalte {e} fehlt im DataFrame f√ºr These 6.")
        return go.Figure().add_annotation(text=f"Daten f√ºr These 6 fehlen. {e}", xref="paper", yref="paper", x=0.5, y=0.5, showarrow=False)

    regime_order = ['Bear', 'Sideways', 'Bull']
    regime_color_map = {'Bear': '#EF553B', 'Sideways': '#FECB52', 'Bull': '#00CC96'}

    fig = px.box(
        df_temp,
        x='Regime',
        y='MA_Diff',
        category_orders={'Regime': regime_order},
        color='Regime',
        color_discrete_map=regime_color_map,
        title='Pr√ºfung These 6: MA50 - MA200 Differenz nach Markt-Regime (Boxplot)',
        labels={'MA_Diff': 'Differenz MA50 - MA200 (USD)', 'Regime': 'Markt-Regime'},
        template='plotly_white'
    )

    # Horizontale Linie bei y=0
    fig.add_hline(y=0, line_width=2, line_dash="dash", line_color="gray")
    fig.update_xaxes(title_text='Markt-Regime')
    fig.update_yaxes(title_text='MA-Differenz (USD)')

    return fig



# ==================================================================================================
# 5. DASHBOARD LOGIK UND LAYOUT
# ==================================================================================================

if df_master is not None:
    
    # ----------------------------------------------------------------------------------------------
    # VORBEREITUNG DER MODELLE UND DATEN
    # ----------------------------------------------------------------------------------------------
    
    reg_models = ['ridge', 'elasticnet', 'svr_rbf', 'random_forest', 'lasso', 'linear', 'naive']
    dir_models = ['LogisticRegression', 'SVM', 'RandomForest', 'KNN', 'GradientBoosting']
    cluster_models = ['KMeans', 'MiniBatchKMeans', 'GMM', 'VBGMM', 'Spectral']

    # Setze Standardwerte f√ºr Selectboxes
    selected_reg_model = reg_models[0]
    selected_dir_model = dir_models[0]
    selected_cluster_model = cluster_models[0]
    
    # --- 4.1 OBERER CONTAINER (Fokus-Elemente mit Datumsbereich-Slider) ---
    st.title("‚Çø BTC Markt-Regime & Prognose-Dashboard")
    
    focus_container = st.container()
    
    with focus_container:
        st.markdown("### üéØ Prognose-Fokus")
        
        # Datums-Vorbereitung f√ºr Slider
        date_timestamps = df_master.index.tolist()
        date_dt_list = [ts.to_pydatetime() for ts in date_timestamps]
        
        num_days = len(date_dt_list)
        if num_days > 50:
            default_start_date = date_dt_list[-51] 
        else:
            default_start_date = date_dt_list[0]
            
        default_end_date = date_dt_list[-1] 
        
        col_slider = st.columns([1])[0] 

        with col_slider:
            # DATUMSBEREICH SLIDER
            selected_date_range = st.slider(
                "W√§hlen Sie einen Datumsbereich (Historie):",
                min_value=date_dt_list[0],  
                max_value=date_dt_list[-1], 
                value=(default_start_date, default_end_date), 
                format="YYYY-MM-DD" 
            )
            start_date, end_date = selected_date_range
            
            fokus_tag = end_date.strftime('%Y-%m-%d')
            st.info(f"Analyse-Zeitraum: **{start_date.strftime('%Y-%m-%d')}** bis **{fokus_tag}** | Fokus-Tag: **{fokus_tag}**")

        st.markdown("---") 
        
    # --- 4.2 Sidebar (Konfiguration & Sekund√§r-Filter) ---
    
    st.sidebar.header("Konfiguration & Filter")
    
    # --- Historie-Filter ---
    st.sidebar.subheader("Historie-Filter")
    alle_regime = df_master['Regime'].unique().tolist()
    ausgewaehlte_regime = st.sidebar.multiselect("Markt-Regime filtern (Tab 1):", options=alle_regime, default=alle_regime)
    if 'Signal' in df_master.columns:
        alle_signale = df_master['Signal'].unique().tolist()
        ausgewaehlte_signale = st.sidebar.multiselect("ML-Signal filtern (Tab 1):", options=alle_signale, default=alle_signale)
    else:
        ausgewaehlte_signale = []
    
    # --- √Ñhnlichkeits-Analyse ---
    st.sidebar.markdown("---")
    st.sidebar.subheader("√Ñhnlichkeits-Analyse (Analogiebildung)")
    # top_k ist hier definiert und wird in find_analogies verwendet
    top_k = st.sidebar.slider("Anzahl √§hnlicher Tage (Top-K):", min_value=1, max_value=100, value=50)

    # --- Modell-Orientierung ---
    st.sidebar.markdown("---")
    st.sidebar.subheader("Modell-Einstellungen")
    st.sidebar.markdown("*(Die Modellauswahl befindet sich jetzt in den jeweiligen 'Metriken'-Tabs.)*")
    
    
    # --- 4.3 Haupt-Layout (Tabs) ---
    
    tab1, tab_reg, tab_dir, tab_cluster, tab2_new, tab4_new, tab5_new = st.tabs([
        "Historie & Signale (Filter)", 
        "Regressions-Metriken (Preisprognose)",
        "Klassifikations-Metriken (Richtung/Signal)",
        "Clustering-Metriken (Markt-Regime)", 
        "Signal-Analyse & Analogien",
        "Markt-Visualisierung (PCA/Cluster)", 
        "Explorative Analyse (EDA)"
    ])
    
    # --- Tab 1: Historie & Signale (Filter) ---
    with tab1:
        st.header("1. Historie & Signale im Zeitverlauf")
        
        # 1. Filterung des Master-DataFrames
        df_filtered = df_master[df_master['Regime'].isin(ausgewaehlte_regime)]
        if 'Signal' in df_master.columns:
             df_filtered = df_filtered[df_filtered['Signal'].isin(ausgewaehlte_signale)]
             
        # 2. Kontext-Slicing
        try:
            df_display_context = df_filtered.loc[start_date:end_date]
        except Exception as e:
            st.error(f"Fehler beim Filtern des Datumsbereichs: {e}. Zeige die letzten 100 Tage an.")
            df_display_context = df_filtered.tail(100)
            
        # -----------------------------------------------------------
        # CANDLESTICK CHART
        # -----------------------------------------------------------
        st.subheader("1.0 BTC/USD Candlestick Chart mit Markt-Regime-Hintergrund")
        
        if not df_display_context.empty:
            chart_fig = create_candlestick_chart(df_display_context)
            st.plotly_chart(chart_fig, use_container_width=True)
        else:
            st.warning("Keine Daten vorhanden, um den Chart im ausgew√§hlten Zeitraum zu erstellen.")
            
        st.markdown("---") 

        
        # 3. Definition der Spalten-Views
        cols_features = ['Open', 'High', 'Low', 'Close', 'Volume', 'Return', 'MA7', 'MA21', 'MA50', 'MA200', 'Volatility30', 'Momentum7']
        cols_cluster = ['Cluster_3', 'Regime', 'PC1', 'PC2']
        cols_signals = ['Direction_Pred', 'Prob_Up', 'Signal', 'Pred_1d', 'Pred_7d', 'Pred_30d', 'Pred_90d', 'Pred_365d']

        
        # --- TEIL 1: TECHNISCHE FEATURES (ROHDATEN) ---
        st.subheader("1.1 Technische Features & Preisdaten")
        df_features = df_display_context[[col for col in cols_features if col in df_display_context.columns]]
        df_features_formatted = format_prices(df_features)
        
        st.dataframe(
            df_features_formatted.style.apply(highlight_focus_day, axis=1, focus_date_str=fokus_tag), 
            use_container_width=True,
            height=300
        )
        st.markdown(
            "### üìà Erkl√§rung: Technische Features (Input f√ºr die Modelle)\n"
            "‚û°Ô∏è **OHLC:** Open, High, Low, Close definieren die Preisspanne.\n"
            "‚û°Ô∏è **Volume & Return:** Gehandeltes Volumen und t√§gliche prozentuale Preis√§nderung.\n"
            "‚û°Ô∏è **MA:** Gleitende Durchschnitte (7/21/50/200) als wichtigste Trendindikatoren.\n"
            "‚û°Ô∏è **Volatilit√§t & Momentum:** Schwankungsbreite (Risiko) und Geschwindigkeit der Preis√§nderung."
        )
        
        st.markdown("---")
        
        # --- TEIL 2: MARKT-REGIME (CLUSTERING) ---
        st.subheader("1.2 Markt-Regime & PCA-Ergebnisse")
        
        df_cluster = df_display_context[[col for col in cols_cluster if col in df_display_context.columns]]
        df_cluster_formatted = format_cluster_pca(df_cluster)

        st.dataframe(
            df_cluster_formatted.style.apply(highlight_focus_day, axis=1, focus_date_str=fokus_tag), 
            use_container_width=True,
            height=300
        )
        st.markdown(
            "### üõ°Ô∏è Erkl√§rung: Markt-Regime (Clustering & Dimension)\n"
            "‚û°Ô∏è **Cluster_ID & Regime:** Numerische Kennung und lesbare Bezeichnung der Marktphase (z.B. 'Bull', 'Bear').\n"
            "‚û°Ô∏è **PC1:** Die **wichtigste** statistische Gr√∂√üe, die den **Trend** (von B√§r zu Bulle) erfasst.\n"
            "‚û°Ô∏è **PC2:** Die zweitwichtigste Gr√∂√üe, die prim√§r die **Volatilit√§t** und das **'Chaos'** im Markt darstellt."
        )
        
        st.markdown("---")
        
        # --- TEIL 3: MODELL-SIGNALE & PROGNOSEN ---
        st.subheader("1.3 Modell-Signale & Preisprognosen")
        
        df_signals = df_display_context[[col for col in cols_signals if col in df_display_context.columns]]
        df_signals_formatted = format_signals(df_signals)

        st.dataframe(
            df_signals_formatted.style.apply(highlight_focus_day, axis=1, focus_date_str=fokus_tag), 
            use_container_width=True,
            height=300
        )
        st.markdown(
            "### üîÆ Erkl√§rung: Modell-Output (Prognosen)\n"
            "‚û°Ô∏è **Direction_Pred / Signal:** Bin√§re Vorhersage und lesbares Handelssignal f√ºr den n√§chsten Tag (Klassifikation).\n"
            "‚û°Ô∏è **Prob_Up:** Modell-Wahrscheinlichkeit daf√ºr, dass der Preis am n√§chsten Tag steigt.\n"
            "‚û°Ô∏è **Pred (1d/7d/30d/90d/365d):** Erwartete absolute Preisprognose in US-Dollar f√ºr den jeweiligen Zukunftshorizont (Regression)."
        )

        
    # --- Tab 3.1: REGRESSIONS-METRIKEN ---
    with tab_reg:
        st.header("3.1 Regressions-Metriken: Wie hoch ist der Anstieg/Abfall (Preisprognose)?")
        
        if df_reg_metrics is None:
            st.warning(f"‚ö†Ô∏è WARNUNG: Metrik-Datei nicht gefunden oder konnte nicht geladen/geparst werden unter `{REGRESSION_METRICS_PATH}`. Bitte Dateiformat pr√ºfen.")
        else:
            col_horiz, col_reg_select = st.columns(2) 
            
            # Hole alle eindeutigen Horizont-Werte f√ºr das Selectbox
            unique_horizons = df_reg_metrics['horizon'].unique().tolist()
            default_index = unique_horizons.index('7d') if '7d' in unique_horizons else 0

            with col_horiz:
                prognose_horizont = st.selectbox(
                    "Prognose-Horizont (Ziel):", 
                    options=unique_horizons, 
                    index=default_index,
                    key="reg_horizon_select" 
                )
            
            with col_reg_select:
                selected_reg_model = st.selectbox(
                    "W√§hlen Sie das zu bewertende Regressionsmodell:",
                    options=reg_models, 
                    index=reg_models.index(selected_reg_model),
                    key="reg_model_tab"
                )
            
            st.subheader("Performance der Modelle auf dem Testset")

            # Filterung funktioniert jetzt dank der Konvertierung in der Lade-Sektion
            df_reg_filtered = df_reg_metrics[df_reg_metrics['horizon'] == prognose_horizont].copy()
            
            if df_reg_filtered.empty:
                st.warning(f"Keine Regressions-Metriken f√ºr den Horizont **{prognose_horizont}** vorhanden.")
            else:
                st.markdown(f"**Performance f√ºr Horizont: {prognose_horizont}**")
                
                df_display = df_reg_filtered.copy()
                # Umbenennung, jetzt dass 'model' vorhanden sein sollte
                df_display.rename(columns={
                    'model': 'Modell', 'rmse': 'RMSE (Durchschnittlicher Fehler in $)', 
                    'mae': 'MAE (Durchschnittliche Abweichung in $)', 
                    'r2': 'R¬≤ (Erkl√§rte Varianz, 1.0 ist perfekt)',
                    'mse': 'MSE' 
                }, inplace=True, errors='ignore')
                
                # Formatierung der Metriken
                df_display['MSE'] = pd.to_numeric(df_display['MSE'], errors='coerce').apply(lambda x: f'{x:,.0f}')
                df_display['RMSE (Durchschnittlicher Fehler in $)'] = pd.to_numeric(df_display['RMSE (Durchschnittlicher Fehler in $)'], errors='coerce').apply(lambda x: f'{x:,.2f}')
                df_display['MAE (Durchschnittliche Abweichung in $)'] = pd.to_numeric(df_display['MAE (Durchschnittliche Abweichung in $)'], errors='coerce').apply(lambda x: f'{x:,.2f}')
                df_display['R¬≤ (Erkl√§rte Varianz, 1.0 ist perfekt)'] = pd.to_numeric(df_display['R¬≤ (Erkl√§rte Varianz, 1.0 ist perfekt)'], errors='coerce').apply(lambda x: f'{x:.4f}')
                
                # Entferne die 'horizon' Spalte f√ºr die Anzeige
                df_display = df_display.drop(columns=['horizon'], errors='ignore')

                st.dataframe(
                    # Anwendung des abgesicherten Stylings
                    df_display.style.apply(highlight_selected_reg_model, axis=1, current_model=selected_reg_model),
                    use_container_width=True,
                    column_order=['Modell', 'RMSE (Durchschnittlicher Fehler in $)', 'MAE (Durchschnittliche Abweichung in $)', 'R¬≤ (Erkl√§rte Varianz, 1.0 ist perfekt)', 'MSE']
                )

                best_mae_row = df_reg_filtered.loc[df_reg_filtered['mae'].idxmin()]
                best_model = best_mae_row['model']
                best_mae = f"{best_mae_row['mae']:,.0f}"
                best_r2 = f"{best_mae_row['r2']:.2f}"
                
                if prognose_horizont in ['1d', '7d']:
                    quality_text = f"Die Prognosequalit√§t f√ºr den kurzen Horizont von **{prognose_horizont}** ist relativ **hoch**."
                elif prognose_horizont in ['30d', '90d']:
                    quality_text = f"Die Prognosequalit√§t f√ºr den mittleren Horizont von **{prognose_horizont}** ist **moderat**."
                else:
                    quality_text = f"Die Prognosequalit√§t f√ºr den langfristigen Horizont von **{prognose_horizont}** ist **gering**."

                st.markdown("---")
                st.subheader("üí° Performance-Analyse und Interpretation (Fokus: Aktueller Horizont)")
                st.info(
                    f"**Ergebnis f√ºr {prognose_horizont}:** {quality_text} Das Modell **{best_model}** liefert aktuell die besten Ergebnisse in der Preisprognose.\n\n"
                    f"‚û°Ô∏è **Mittlere Abweichung (MAE):** Der durchschnittliche absolute Fehler des besten Modells betr√§gt **{best_mae} $**.\n"
                    f"‚û°Ô∏è **Erkl√§rte Varianz (R¬≤):** Das Modell **{best_model}** erkl√§rt **{best_r2}** der beobachteten Preisbewegungen."
                )
            
            st.markdown("---")
            st.subheader("Definition der Kennzahlen")
            st.info(
                "**RMSE** & **MAE** (Fehler in $): Der mittlere Vorhersagefehler. **Niedrige Werte sind besser**.\n\n"
                "**MSE** (Mean Squared Error): Quadratischer Fehler. **Niedrige Werte sind besser**.\n\n"
                "**R¬≤** (Erkl√§rte Varianz): Anteil der Varianz, der durch das Modell erkl√§rt wird. **Werte nahe 1.0 sind ideal**."
            )
            
            st.markdown("--")
                            # NEU EINGEF√úGTE ERKL√ÑRUNG F√úR LAIEN:
            st.subheader("2.1 Erkl√§rungen & Interpretation")
            st.markdown("""
                ## üí° Erkl√§rungen und Benchmarks: Regressions-Metriken (Preisprognose)

                ### üìä Fehler-Metriken (RMSE & MAE)
                Diese Werte messen, wie weit unser Modell in **US-Dollar ($)** mit der Preisvorhersage **daneben** liegt. Niedrigere Werte sind **immer** besser!

                * **MAE (Mean Absolute Error):** Der **durchschnittliche absolute Fehler**. Zeigt den tats√§chlichen, durchschnittlichen Irrtum in $.
                * **RMSE (Root Mean Squared Error):** √Ñhnlich wie MAE, aber **gro√üe Fehler** (Ausrei√üer) werden **st√§rker gewichtet und bestraft** (quadriert).

                #### Was ist "Niedrig" bei BTC? (Kontextabh√§ngig)
                Da der Bitcoin-Preis stark schwankt, beurteilen wir den Fehler **relativ** zum Preisniveau:
                * ‚úÖ **Sehr gut:** Der Fehler (MAE/RMSE) betr√§gt **weniger als 1%** des aktuellen Bitcoin-Preises.
                * üü° **Akzeptabel:** Der Fehler liegt zwischen 1% und 3%.

                ---

                ### üìà G√ºte-Metrik (R¬≤)
                Der $\mathbf{R^2}$ (R-Quadrat oder **Erkl√§rte Varianz**) sagt uns, **wie gut** unser Modell die historischen Preisbewegungen **erfasst und erkl√§ren** kann.

                * **R¬≤ nahe 1.0:** Das Modell erkl√§rt fast die gesamte Schwankung der Preise ‚Äì **Idealfall**.
                * **R¬≤ nahe 0.0:** Das Modell ist nicht besser, als einfach den Durchschnittspreis zu raten.

                #### Was ist "Gut" bei BTC-Prognosen?
                F√ºr komplexe, langfristige Finanzprognosen ist ein hoher $R^2$ schwer zu erreichen:
                * ‚≠ê **Sehr gut:** $\mathbf{R^2 > 0.7}$
                * üëç **Akzeptabel:** $\mathbf{R^2 > 0.5}$

                ---

                ## üîé **Beispiel-Analyse: Ridge (1 Tag Horizont)**

                Angenommen, Sie haben das Modell **Ridge** und den Horizont **1 Tag** ausgew√§hlt, und die Tabelle zeigt folgende fiktive Ergebnisse (bei einem aktuellen BTC-Preis von $65.000$):

                | Modell | RMSE (Durchschn. Fehler in $) | MAE (Durchschn. Abweichung in $) | R¬≤ (Erkl√§rte Varianz) |
                |---|---|---|---|
                | **Ridge** | **$450.00** | **$320.00** | **0.8875** |

                ### Interpretation dieser Beispiel-Zahlen:

                1. ¬†**R¬≤ (G√ºte):** Mit $\mathbf{0.8875}$ ist die erkl√§rte Varianz **extrem hoch** (nahe 1.0).
                ¬† ¬† > **Fazit:** ‚≠ê Das Ridge-Modell f√§ngt fast $\mathbf{89\%}$ der Preisschwankungen am n√§chsten Tag korrekt ein. Dies deutet auf eine **ausgezeichnete Prognoseg√ºte** hin.

                2. ¬†**MAE (Durchschnittlicher Fehler):** Der durchschnittliche Fehler liegt bei **$320.00$**.
                ¬† ¬† > **Fazit:** Bei einem Preis von $65.000$ entspricht dies einem Fehler von nur $\approx 0.49\%$. Dies liegt **weit unter der 1%-Benchmark**. ‚úÖ Das Modell liegt im Durchschnitt sehr nah am tats√§chlichen Preis.

                3. ¬†**RMSE (Gewichteter Fehler):** Der RMSE von **$450.00$** ist h√∂her als der MAE.
                ¬† ¬† > **Fazit:** Die Differenz zwischen RMSE ($450$) und MAE ($320$) ist relativ gro√ü. Das bedeutet, dass es gelegentlich **gro√üe Ausrei√üer** (fehlerhafte Prognosen) gibt, die den RMSE st√§rker nach oben ziehen.
                                            """, 
                        unsafe_allow_html=True # Notwendig f√ºr die Markdown-Formatierung
                    )
                # ENDE DER NEUEN ERKL√ÑRUNG


# --- Tab 3.2: KLASSIFIKATIONS-METRIKEN ---
with tab_dir:
    st.header("3.2 Klassifikations-Metriken: Steigt oder f√§llt der Preis **(Direction/Signal f√ºr den n√§chsten Tag)**?")
    
    col_dir_select, col_empty = st.columns([1, 4])
    with col_dir_select:
        selected_dir_model = st.selectbox(
            "W√§hlen Sie das zu bewertende Klassifikationsmodell:",
            options=dir_models, 
            index=dir_models.index(selected_dir_model),
            key="dir_model_tab"
        )
        
    st.subheader("Performance der Modelle auf dem Testset")
    
    if df_dir_metrics is None:
        st.warning(f"‚ö†Ô∏è WARNUNG: Metrik-Datei nicht gefunden oder konnte nicht geladen werden unter `{DIRECTION_METRICS_PATH}`.")
    else:
        st.success("‚úÖ Klassifikations-Metrik-Datei gefunden und geladen.")
        
        # üö® KRITISCHE KORREKTUR: Spaltennamen auf Gro√üschreibung anpassen (behebt KeyError: 'Recall')
        # Wir stellen sicher, dass die Namen, die der Analysecode erwartet, existieren.
        rename_map_analysis = {
            'precision': 'Precision',
            'recall': 'Recall', # <--- FIX F√úR DEN FEHLER
            'f1': 'F1',
            'f1-score': 'F1',
            'f1_score': 'F1',
            'accuracy': 'Accuracy'
        }
        
        # Wende die Umbenennung auf den Haupt-DataFrame an, um den Analyseblock zu reparieren
        df_dir_metrics.rename(columns=rename_map_analysis, inplace=True, errors='ignore')
        
        # --- Start des Display-Codes ---
        df_dir_display = df_dir_metrics.copy()
        
        # Stelle sicher, dass die Spalte 'Model' oder 'Modell' existiert
        if 'Model' in df_dir_display.columns:
            df_dir_display.rename(columns={'Model': 'Modell'}, inplace=True, errors='ignore')
        elif 'model' in df_dir_display.columns:
             df_dir_display.rename(columns={'model': 'Modell'}, inplace=True, errors='ignore')
        
        # Die Umbenennung f√ºr die Anzeige kann jetzt die Gro√übuchstaben verwenden
        df_dir_display.rename(columns={
            'Accuracy': 'Accuracy (Gesamttreffer)', 
            'Precision': 'Precision (Zuverl√§ssigkeit)', 'Recall': 'Recall (Erkennung)', 
            'F1': 'F1-Score (Balance)' 
        }, inplace=True, errors='ignore')

        # Korrektur: Konvertierung in Prozent f√ºr bessere Lesbarkeit
        for col_key in ['Accuracy (Gesamttreffer)', 'Precision (Zuverl√§ssigkeit)', 'Recall (Erkennung)', 'F1-Score (Balance)']: 
            if col_key in df_dir_display.columns:
                df_dir_display[col_key] = pd.to_numeric(df_dir_display[col_key], errors='coerce')
                df_dir_display[col_key] = df_dir_display[col_key].apply(lambda x: f'{x*100:.2f} %') 
        
        st.dataframe(
            # Anwendung des abgesicherten Stylings
            df_dir_display.style.apply(highlight_selected_dir_model, axis=1, current_model=selected_dir_model),
            use_container_width=True
        )
        
        # W√§hle die beste Zeile basierend auf F1-Score
        if 'F1' in df_dir_metrics.columns: # F1 ist jetzt durch die Korrektur gesichert
            best_f1_row = df_dir_metrics.loc[df_dir_metrics['F1'].idxmax()]
            
            # Stelle sicher, dass 'Modell' in der besten Zeile verf√ºgbar ist
            # Da wir 'Modell' in df_dir_metrics nicht umbenannt haben, m√ºssen wir hier auf die Originalspalte pr√ºfen
            best_model_dir = next((best_f1_row[col] for col in ['Modell', 'Model', 'model'] if col in best_f1_row), "N/A")

            # Diese Zeilen funktionieren jetzt, da 'Recall', 'Precision' und 'F1' Gro√übuchstaben sind
            best_f1 = f"{best_f1_row['F1'] * 100:.2f}%"
            best_recall = f"{best_f1_row['Recall'] * 100:.2f}%"
            best_precision = f"{best_f1_row['Precision'] * 100:.2f}%"
            
            st.markdown("---")
            st.subheader("üí° Performance-Analyse und Interpretation (Fokus: Direction-Signal 1d)")
            st.info(
                f"**Ergebnis:** Das Modell **{best_model_dir}** bietet mit einem F1-Score von **{best_f1}** die beste Gesamtbalance.\n\n"
                f"‚û°Ô∏è **Erkennung (Recall):** Das Modell identifiziert **{best_recall}** aller tats√§chlichen Preisanstiege (minimiert verpasste Kaufgelegenheiten).\n"
                f"‚û°Ô∏è **Zuverl√§ssigkeit (Precision):** Nur in **{best_precision}** der F√§lle liegt das Signal richtig, wenn es einen Anstieg vorhersagt (Fehlalarmrate beachten)."
            )
        else:
            # Info-Meldung angepasst, falls F1 aus unbekannten Gr√ºnden immer noch fehlt
            st.info("üí° **Analyse-Status:** Die Performance-Interpretation ist nicht verf√ºgbar. √úberpr√ºfen Sie, ob die Spalte 'F1' in der CSV existiert.")
        
        st.markdown("---")
        st.subheader("Definition der Kennzahlen")
        st.info(
            "**Accuracy** (Genauigkeit): Gesamtanteil korrekter Vorhersagen. **Werte nahe 100% sind ideal**.\n\n"
            
            "**Precision** (Zuverl√§ssigkeit): Zuverl√§ssigkeit des Signals. Wie oft liegt das Modell richtig, wenn es einen Anstieg vorhersagt? "
            "**Beispiel:** Eine Precision von **60%** bedeutet, dass 6 von 10 'Kauf'-Signalen (tats√§chlich) zum Erfolg f√ºhren. **Hohe Precision** minimiert Fehlsignale (Fehlalarme).\n\n"
            
            "**Recall** (Vollst√§ndigkeit/Sensitivit√§t): Vollst√§ndigkeit der Erkennung. Wie viele tats√§chliche Anstiege wurden vom Modell erkannt? "
            "**Beispiel:** Ein Recall von **85%** bedeutet, dass das Modell 8,5 von 10 echten Preisanstiegen identifiziert. **Hoher Recall** minimiert verpasste Chancen.\n\n"
            
            "**F1-Score**: Bester Indikator f√ºr die Gesamtperformance (Harmonisches Mittel von Precision und Recall). Besonders wichtig bei ungleich verteilten Klassen.\n\n"
            
            "**Support** (Vorkommen): Die absolute Anzahl der Instanzen der jeweiligen Klasse (z.B. 'Up' oder 'Down') im Testdatensatz. Dient zur Bewertung der **G√ºltigkeit** der Metriken und zeigt die **Klassen-Imbalance** auf."
        )
    # --- Tab 3.3: CLUSTERING-METRIKEN ---
    with tab_cluster:
        st.header("3.3 Clustering-Metriken: In welchem Trend/Markt-Regime bewegen wir uns **(Gesamte Historie)**?")
        
        col_cluster_select, col_empty = st.columns([1, 4])
        with col_cluster_select:
            selected_cluster_model = st.selectbox(
                "W√§hlen Sie das zu bewertende Clustering-Modell:",
                options=cluster_models, 
                index=cluster_models.index(selected_cluster_model),
                key="cluster_model_tab"
            )
            
        st.subheader("Clustering-Metriken (Interne Validierung)")
        
        if df_cluster_metrics is None:
             st.warning(f"‚ö†Ô∏è WARNUNG: Metrik-Datei nicht gefunden unter `{CLUSTERING_METRICS_PATH}`.")
        else:
            st.success("‚úÖ Clustering-Metrik-Datei gefunden und geladen.")
            
            df_cluster_metrics.rename(columns={
                'model': 'Modell', 'Model': 'Modell',
                'Silhouette': 'Silhouette Score', 'Silhouette_Score': 'Silhouette Score', 'silhouette_score': 'Silhouette Score',
                'DaviesBouldin': 'Davies-Bouldin Index', 'Davies_Bouldin_Index': 'Davies-Bouldin Index', 'davies_bouldin_index': 'Davies-Bouldin Index'
            }, inplace=True, errors='ignore') 
            
            required_cols = ['Modell', 'Silhouette Score', 'Davies-Bouldin Index']
            if all(col in df_cluster_metrics.columns for col in required_cols):
                
                st.markdown("**Performance der Clustering-Modelle**")
                
                # Formatierung f√ºr die Anzeige
                df_cluster_metrics['Silhouette Score'] = pd.to_numeric(df_cluster_metrics['Silhouette Score'], errors='coerce').apply(lambda x: f'{x:.3f}')
                df_cluster_metrics['Davies-Bouldin Index'] = pd.to_numeric(df_cluster_metrics['Davies-Bouldin Index'], errors='coerce').apply(lambda x: f'{x:.3f}')

                st.dataframe(
                    # Anwendung des abgesicherten Stylings
                    df_cluster_metrics.style.apply(highlight_selected_cluster_model, axis=1, current_model=selected_cluster_model),
                    use_container_width=True
                )
                
                # Hier nehmen wir an, dass die Spalten numerisch sind, um den besten Index zu finden
                df_cluster_metrics['Silhouette Score Num'] = pd.to_numeric(df_cluster_metrics['Silhouette Score'].str.replace(',', '.'), errors='coerce')
                
                best_cluster_row = df_cluster_metrics.loc[df_cluster_metrics['Silhouette Score Num'].idxmax()]
                best_cluster_model = best_cluster_row['Modell']
                best_silhouette = best_cluster_row['Silhouette Score']
                best_db_index = best_cluster_row['Davies-Bouldin Index']
    
                st.markdown("---")
                st.subheader("üí° Performance-Analyse und Interpretation (Fokus: Markt-Regime-Bildung)")
                st.info(
                    f"**Ergebnis:** Das Modell **{best_cluster_model}** liefert mit einem Silhouette Score von **{best_silhouette}** die klarste und robusteste Unterscheidung der Markt-Regime.\n\n"
                    f"‚û°Ô∏è **Silhouette Score ({best_silhouette}):** **Werte nahe 1.0 sind ideal** (beste Cluster-Qualit√§t).\n"
                    f"‚û°Ô∏è **Davies-Bouldin Index ({best_db_index}):** **Niedrigere Werte (nahe 0.0) sind besser** (beste Cluster-Trennung)."
                )
            else:
                 st.info("üí° **Analyse-Status:** Die Performance-Interpretation ist nicht verf√ºgbar, da wichtige Metrik-Spalten fehlen. Bitte pr√ºfen Sie die Spaltennamen in der geladenen Metrik-Datei.")

            # st.markdown("---")
            # st.subheader("Definition der Kennzahlen")
            # st.info(
            #     "**Silhouette Score**: Misst die Kompaktheit und Trennung der Cluster. **Werte nahe 1.0 sind ideal**.\n"
            #     "**Davies-Bouldin Index**: Misst die √Ñhnlichkeit zwischen den Clustern. **Werte nahe 0.0 sind ideal**."
            # )
            
            st.markdown("---")
            st.subheader("Definition der Kennzahlen")
            st.info(
                "Die G√ºte von Clustering-Ergebnissen wird durch **interne Validierungs-Indizes** bewertet, da keine wahren Labels (Ground Truth) existieren. Diese Indizes messen, wie **kompakt** die einzelnen Cluster sind (Homogenit√§t) und wie **klar voneinander getrennt** die Cluster sind (Separierbarkeit)."
            )
            
            st.markdown("---")
            
            ### üìè Silhouette Score
            st.markdown("#### üìè Silhouette Score (Kompaktheit & Trennung)")
            st.markdown(
                "Der **Silhouette Score** misst, wie √§hnlich ein Objekt seinem eigenen Cluster ist (Kompaktheit) im Vergleich zu anderen Clustern (Trennung). Er liegt zwischen **-1 und +1**.\n\n"
                "‚û°Ô∏è **Werte nahe +1.0** sind **ideal** und bedeuten, dass das Objekt gut zu seinem Cluster passt und von den Nachbarclustern gut getrennt ist.\n"
                "‚û°Ô∏è **Werte nahe 0** deuten auf √ºberlappende Cluster hin.\n"
                "‚û°Ô∏è **Negative Werte** zeigen an, dass ein Objekt dem falschen Cluster zugewiesen wurde."
            )
            
            ### ‚öñÔ∏è Calinski-Harabasz Index
            st.markdown("#### ‚öñÔ∏è Calinski-Harabasz Index (Varianz-Verh√§ltnis)")
            st.markdown(
                "Der **Calinski-Harabasz Index** (auch **Varianz-Verh√§ltnis-Kriterium**) bewertet das Verh√§ltnis von **Zwischen-Cluster-Varianz** (B) zu **Innerhalb-Cluster-Varianz** (W).\n\n"
                "$$CH = \\frac{\\text{Varianz zwischen Clustern (B)}}{\\text{Varianz innerhalb der Cluster (W)}}$$\n\n"
                "‚û°Ô∏è **Ein h√∂herer Wert ist ideal**, da er eine klare Trennung der Cluster (hohes B) und gleichzeitig kompakte Cluster (kleines W) signalisiert."
            )
            
            ### üß© Davies-Bouldin Index
            st.markdown("#### üß© Davies-Bouldin Index (√Ñhnlichkeit der Cluster)")
            st.markdown(
                "Der **Davies-Bouldin Index** (DBI) ist definiert als der Durchschnitt der maximalen √Ñhnlichkeit zwischen jedem Cluster und seinem √§hnlichsten Nachbarcluster. Er basiert auf dem Verh√§ltnis des durchschnittlichen Abstands der Punkte innerhalb eines Clusters zum Abstand zwischen den Cluster-Zentren.\n\n"
                "‚û°Ô∏è **Werte nahe 0.0** sind **ideal** und weisen auf klar getrennte und kompakte Cluster hin. Er ist besonders n√ºtzlich, um die interne Kompaktheit zu bewerten."
            )
            
# --- Tab 2: Signal-Analyse & Analogien ---
with tab2_new:
    st.header("2. Signal-Analyse & Analogien")
    
    # Holen Sie sich das Signal des Fokus-Tages
    focus_signal = df_master.loc[fokus_tag, 'Signal'] if fokus_tag in df_master.index and 'Signal' in df_master.columns else "N/A"
    focus_regime = df_master.loc[fokus_tag, 'Regime'] if fokus_tag in df_master.index and 'Regime' in df_master.columns else "N/A"
    
    st.subheader(f"üéØ Fokus-Tag: **{fokus_tag}** (Regime: **{focus_regime}** | Signal: **{focus_signal}**)")
    st.caption(f"√Ñhnlichkeitsanalyse basiert auf den Top **{top_k}** √§hnlichsten Tagen.")

    # -----------------------------------------------------------
    # TEIL 1: √ÑHNLICHKEITSANALYSE (ANALOGIEN)
    # -----------------------------------------------------------
    
    focus_date_dt = pd.to_datetime(fokus_tag)
    
    # --- 1. Durchf√ºhrung der √Ñhnlichkeitsanalyse ---
    df_analogies, summary_metrics = find_analogies(df_master, focus_date_dt, top_k)
    
    st.markdown("---")
    st.subheader("2.1 Historische Analogien (Was passierte nach √§hnlichen Tagen?)")
    
    # ************************************************************
    # HIER BEGINNT DIE ZENTRALE IF-ELSE-ABFRAGE
    # ************************************************************
    if not df_analogies.empty and summary_metrics['Tage analysiert'] > 0:
        
        # --- ZUSAMMENFASSUNG METRIKEN ---
        summary_df = pd.DataFrame([summary_metrics]).T
        summary_df.columns = ['Ergebnis']
        
        summary_df.iloc[0, 0] = f"{int(summary_df.iloc[0, 0])} Tage"
        summary_df.iloc[1, 0] = f"{summary_df.iloc[1, 0]:.2f} %"
        summary_df.iloc[2, 0] = f"{summary_df.iloc[2, 0]:.2f} %"
        summary_df.iloc[3, 0] = f"{summary_df.iloc[3, 0]:.2f} %"

        st.dataframe(summary_df, use_container_width=True)
        
        # --- DYNAMISCHE INTERPRETATION ---
        anteil_up = summary_metrics['Anteil Up (7d)']
        avg_return = summary_metrics['Durchschn. 7d Return (Real)']
        
        st.markdown("---")
        st.subheader("Interpretation der historischen Unterst√ºtzung")
        
        if focus_signal == 'üìà Up':
            if anteil_up >= 70:
                analogy_interpretation = f"üëç **Starke Bullish-Best√§tigung:** In **{anteil_up:.0f}%** der √§hnlichen F√§lle stieg der Preis. Der durchschnittliche reale Zuwachs betrug **{avg_return:.2f}%**. Die Historie **unterst√ºtzt** das 'Up'-Signal stark."
                st.success(analogy_interpretation)
            elif anteil_up <= 30:
                analogy_interpretation = f"üëé **Widerspruch (Fehlalarm-Risiko):** Nur in **{anteil_up:.0f}%** der F√§lle stieg der Preis. Der durchschnittliche reale Return ist **{avg_return:.2f}%**. Die Historie **widerlegt** das 'Up'-Signal stark."
                st.error(analogy_interpretation)
            else:
                analogy_interpretation = f"‚ö†Ô∏è **Unklare Unterst√ºtzung:** Die Historie ist mit **{anteil_up:.0f}%** Up-Tagen gemischt. **Vorsicht** ist geboten, da die Analogie keine klare Richtung liefert."
                st.warning(analogy_interpretation)

        elif focus_signal == 'üìâ Down':
            if anteil_up <= 30:
                analogy_interpretation = f"üëç **Starke Bearish-Best√§tigung:** Nur in **{anteil_up:.0f}%** der √§hnlichen F√§lle stieg der Preis (d.h. er fiel in {100-anteil_up:.0f}% der F√§lle). Der durchschnittliche reale Return ist **{avg_return:.2f}%** (meist negativ). Die Historie **unterst√ºtzt** das 'Down'-Signal stark."
                st.success(analogy_interpretation)
            elif anteil_up >= 70:
                analogy_interpretation = f"üëé **Widerspruch (Hohes Risiko):** In **{anteil_up:.0f}%** der F√§lle stieg der Preis, obwohl das Modell 'Down' sagt. Der durchschnittliche reale Return ist **{avg_return:.2f}%** (meist positiv). Die Historie **widerlegt** das 'Down'-Signal stark."
                st.error(analogy_interpretation)
            else:
                analogy_interpretation = f"‚ö†Ô∏è **Unklare Unterst√ºtzung:** Die Historie ist mit **{anteil_up:.0f}%** Up-Tagen gemischt. **Vorsicht** ist geboten, da die Analogie keine klare Richtung liefert."
                st.warning(analogy_interpretation)
        else:
             analogy_interpretation = "‚ö†Ô∏è **Kein klares Signal:** Das ML-Modell lieferte kein klares Up/Down-Signal, daher kann die Analogie nur die historische Verteilung zeigen."
             st.warning(analogy_interpretation)

        st.markdown("---")
        st.markdown(f"### üìã Details der Top {top_k} Analogien")
        
        analogy_cols = ['Regime', 'PC1', 'PC2', 'Distance', 'Close', 'Return', 'Real_Return_7d']
        
        df_analogy_display = df_analogies[[col for col in analogy_cols if col in df_analogies.columns]].copy()
        
        # Formatting for display
        df_analogy_display['PC1'] = df_analogy_display['PC1'].apply(lambda x: f'{x:.3f}')
        df_analogy_display['PC2'] = df_analogy_display['PC2'].apply(lambda x: f'{x:.3f}')
        df_analogy_display['Close'] = df_analogy_display['Close'].apply(lambda x: f'{x:,.2f}')
        df_analogy_display['Return'] = df_analogy_display['Return'].apply(lambda x: f'{x:.4f}')
        # Korrektur: Formatiere den tats√§chlichen Return in Prozent
        df_analogy_display['Real_Return_7d'] = df_analogy_display['Real_Return_7d'].apply(lambda x: f'{x*100:.2f} %') 
        df_analogy_display['Distance'] = df_analogy_display['Distance'].apply(lambda x: f'{x:.3f}')
        
        df_analogy_display.rename(columns={
            'Distance': '√Ñhnlichkeit (Abstand)',
            'Real_Return_7d': 'Tats√§chlicher 7d Return (Real)',
            'Close': 'Schlusskurs',
            'Return': 'T√§gl. Return'
        }, inplace=True)
        
        # Wende das Highlight-Styling auf die Spalte an
        st.dataframe(
            df_analogy_display.style.apply(highlight_return_analogy, axis=0).apply(highlight_focus_day, axis=1, focus_date_str=fokus_tag), 
            use_container_width=True
        )
        
        # NEUE ERKL√ÑRUNG DER ZAHLEN UNTER DER TABELLE (ULTIMATIV KORRIGIERT)
        st.markdown("### üìä Bedeutung der Kennzahlen in der Analogie-Tabelle")
        st.info(
            "Die Analogiebildung basiert auf der **Hauptkomponentenanalyse (PCA)**, welche die Marktstruktur (Indikatoren wie MAs, Momentum und Volatilit√§t) auf zwei Hauptachsen **PC1** und **PC2** verdichtet. "
            "Der gesuchte **euklidische Abstand** ist die direkte, geometrische Distanz zwischen dem **Fokus-Tag** und einem historischen Tag im **PCA-Raum** (der zweidimensionalen Landkarte). **Je niedriger der Abstand, desto √§hnlicher sind die Tage.**"
        )

        st.markdown("---")

        # Erkl√§rung des K-Sliders
        st.markdown("#### ‚öôÔ∏è Der Top-K Slider (Anzahl √§hnlicher Tage)")
        st.markdown(
            "Der Slider **Anzahl √§hnlicher Tage (Top-K)** bestimmt, wie viele historische Tage mit dem **geringsten Abstand** (der h√∂chsten √Ñhnlichkeit) zum Fokus-Tag in der Tabelle angezeigt werden.\n\n"
            "‚û°Ô∏è **Niedrige K-Werte (z.B. K=1 bis 10):** Fokussiert sich auf die **st√§rksten** Analogien. Die Ergebnisse sind pr√§ziser, aber sensibler gegen√ºber Ausrei√üern.\n"
            "‚û°Ô∏è **Hohe K-Werte (z.B. K=50 bis 100):** Liefert einen **durchschnittlichen** Eindruck des historischen Verhaltens unter *√§hnlichen* Marktbedingungen. Gl√§ttet Extremwerte, verw√§ssert aber die st√§rksten Signale."
        )

        st.markdown("---")
                    
        st.markdown("#### Kennzahlen in der Tabelle")

        # 1. √Ñhnlichkeit (Abstand)
        st.markdown("##### 1. √Ñhnlichkeit (Abstand)")
        st.markdown(
            "Dies ist der **euklidische Abstand** zwischen dem **Fokus-Tag** und diesem historischen Tag im PCA-Raum (PC1/PC2). Dieser Wert ist die **Basis** f√ºr das Ranking.\n"
            "‚û°Ô∏è **Idealer Wert:** **Niedriger** (Nahe 0.0) ist besser, da dies eine st√§rkere Korrelation der Marktstrukturen signalisiert."
        )
                    
        # 2. PC1 / PC2 (Marktposition) - FINAL KORRIGIERT
        st.markdown("##### 2. PC1 / PC2 (Marktposition)")
        st.markdown(
            "Diese Werte sind die Koordinaten der **Marktposition** an diesem historischen Tag im PCA-Raum.\n"
            "Die Komponenten fassen Indikatoren wie `MAs`, `Momentum` und `Volatilit√§t` zusammen.\n\n"
            "**Aussage am Markt:**\n"
            "* **PC1 (Trend/Struktur):** Repr√§sentiert die **Bullische Dynamik** (X-Achse). Hohe positive Werte bedeuten einen starken Aufw√§rtstrend (z.B. Preis √ºber `MA50`/`MA200`).\n"
            "* **PC2 (Volatilit√§t/Chaos):** Repr√§sentiert die **Schwankungsbreite** (Y-Achse). Hohe Werte bedeuten turbulente, unsichere M√§rkte (hohe `Volatility30`).\n\n"
            "‚û°Ô∏è **Bedeutung:** Die PC1/PC2-Werte in der Tabelle sollten **sehr nahe** an den Werten des Fokus-Tages liegen. Das best√§tigt, dass die Analogie **strukturell** passt. "
        )
                    
        # 3. Tats√§chlicher 7d Return (Real)
        st.markdown("##### 3. Tats√§chlicher 7d Return (Real)")
        st.markdown(
            "Dies ist die **tats√§chliche prozentuale Preis√§nderung**, die **in den 7 Tagen nach** diesem historischen Tag (der Analogie) eingetreten ist.\n"
            "‚û°Ô∏è **Interpretation:** Die Spalte dient zur Prognose. Wenn die Mehrheit der Top-K-Tage einen **positiven** Return zeigte, deutet dies auf eine historische Wahrscheinlichkeit f√ºr einen Preisanstieg in der kommenden Woche hin."
        )

        st.markdown("---") # Visuelle Trennung des Erkl√§rungsblocks
        
        # -----------------------------------------------------------
        # TEIL 2: SIGNAL-ANALYSE (ERKL√ÑRUNG)
        # -----------------------------------------------------------
        
        st.markdown("---")
        st.subheader("2.2 Erkl√§rung der Logik und Interpretation")
        
        st.markdown(
            "### üîé Signal-Analyse: Was bedeutet das?\n"
            "Dieser Tab verbindet die rohe **Prognose** des Machine-Learning-Modells mit der **historischen Erfahrung** (Analogien).\n\n"
            "#### 1. Der ML-Prognose-Fokus (Signal des Fokus-Tages)\n"
            f"Das ML-Modell hat f√ºr den **Fokus-Tag ({fokus_tag})** das Signal **{focus_signal}** mit dem Markt-Regime **{focus_regime}** ausgegeben.\n"
            "Dieses Signal basiert auf der 1-Tages-Prognose der Klassifikationsmodelle, die die aktuellen **technischen Kennzahlen** bewerten.\n\n"
            "#### 2. Die Analogie-Validierung (Historischer Vergleich)\n"
            "Die **Analogien** suchen mithilfe des **PCA-Markt-Regime-Raums** (`PC1`, `PC2`) nach historischen Tagen, die dem Fokus-Tag mathematisch am √§hnlichsten waren.\n\n"
            "‚û°Ô∏è **Pr√ºfschritt:** Was geschah **historisch** in den 7 Tagen, nachdem der Markt in der gleichen Konstellation wie heute war?\n\n"
            "#### 3. Interpretation der Ergebnisse (Ihre Frage)\n"
            "Die **Metriken** der Analogien dienen als **Konfidenz-Filter** f√ºr die ML-Prognose (siehe die Zusammenfassung oben)."
        )
    
    
# --- Tab 4: MARKT-VISUALISIERUNG (PCA/Cluster) ---
    with tab4_new:
        st.header("4. Markt-Visualisierung (PCA/Cluster)")
        st.markdown("### üó∫Ô∏è Die Position des Fokus-Tags im Markt-Regime-Raum")
        
        # LOKALE √úBERSCHREIBUNG: Eindeutige Datumsauswahl nur f√ºr die PCA
        st.markdown("---")
        
        # *WICHTIG:* Wir nutzen das global definierte end_date als Standardwert
        # und die bereits vorbereitete Liste date_dt_list
        fokus_datum_pca = st.date_input(
            "üìÜ W√§hlen Sie den FOKUS-TAG f√ºr die PCA-Visualisierung:",
            value=end_date, # Nutzt das Enddatum des globalen Sliders als Standard
            min_value=date_dt_list[0],
            max_value=date_dt_list[-1],
            key='fokus_pca_selector'
        )
        
        # Definiere den lokalen Fokus-Tag, der nur hier verwendet wird
        fokus_tag_pca = fokus_datum_pca.strftime('%Y-%m-%d')

        st.info(f"Visualisierter Fokus-Tag: **{fokus_tag_pca}**")
        st.markdown("---")
        
        # -----------------------------------------------------------------
        # NEUER ERKL√ÑRUNGSBLOCK (OPTIMIERT F√úR PR√ÑSENTATION)
        # -----------------------------------------------------------------
        st.subheader("Detaillierte Erl√§uterung der Marktstruktur-Visualisierung")
        st.markdown(
            "Diese Ansicht basiert auf der **Hauptkomponentenanalyse (PCA)**. Die PCA reduziert die Komplexit√§t von √ºber zwanzig technischen Indikatoren auf zwei Hauptachsen. "
            "Das Ergebnis ist eine **zweidimensionale Landkarte** des Marktes, die es uns erm√∂glicht, die aktuelle Marktphase (**Stern/Target**) im Kontext der gesamten historischen Bewegung visuell einzuordnen."
        )
        
        st.markdown("---")
        
        st.subheader("1. Die drei Komponenten der Visualisierung")
        st.markdown(
            "#### a) Datenbasis (Die Historischen Punkte)\n"
            "**Jeder einzelne Punkt** auf der Grafik repr√§sentiert die **Marktstruktur eines Handelstages** in unserem Datensatz. Diese Punkte bilden die **historische Datenbank** aller jemals aufgetretenen Marktbedingungen."
        )
        st.markdown(
            "#### b) Markt-Regime (Die Farb-Cluster)\n"
            "Die Punkte sind mithilfe von Clustering-Algorithmen in **farbige Cluster** (Regime) gruppiert. Ein Cluster fasst Tage mit **statistisch √§hnlicher technischer Struktur** zusammen. "
            "**Fazit:** Tage innerhalb desselben Clusters teilen typischerweise √§hnliche Verhaltensmuster, was f√ºr die Prognose essenziell ist."
        )
        st.markdown(
            f"#### c) Der Fokus-Tag (‚≠ê/üéØ Target)\n"
            f"Der gro√üe, hervorgehobene **Stern** (**‚≠ê** oder **üéØ**) zeigt die **exakte Position** des aktuell gew√§hlten **Fokus-Tages ({fokus_tag_pca})** im Markt-Regime-Raum. "
            "Seine Position im Verh√§ltnis zu den Clustern best√§tigt das von unserem Modell zugewiesene Markt-Regime und dient als visueller Startpunkt f√ºr die Analogien-Analyse."
        )
        
        # -----------------------------------------------------------------
        # VISUALISIERUNG UND ACHSEN-ERKL√ÑRUNG
        # -----------------------------------------------------------------

        pca_fig = create_pca_scatter_plot(df_master, fokus_tag_pca)
        st.plotly_chart(pca_fig, use_container_width=True)

        st.markdown("---")
        st.subheader("2. Interpretation der Achsen (PC1 & PC2)")
        st.markdown(
            "‚û°Ô∏è **X-Achse (PC1): Trend-Komponente (Dominante Marktrichtung).** Repr√§sentiert die prim√§re St√§rke des Trends. "
            "Bewegung nach **rechts** signalisiert eine Zunahme der **bullischen Dynamik** (starker Aufw√§rtstrend). Bewegung nach **links** signalisiert eine starke **bearishe** Tendenz (Abw√§rtstrend).\n\n"
            
            "‚û°Ô∏è **Y-Achse (PC2): Volatilit√§ts-Komponente (Markt-Unsicherheit).** Repr√§sentiert die Schwankungsbreite und das Rauschen im Markt. "
            "Eine **hohe** Position (oben) deutet auf hohe **Volatilit√§t** und Unsicherheit hin. Niedrige Positionen (unten) stehen f√ºr ruhige, stabile Marktphasen.\n\n"
            
            f"‚û°Ô∏è **Stern/Target (‚≠ê/üéØ):** Seine Position in diesem Koordinatensystem liefert die Grundlage f√ºr die Analogie-Suche in **Tab 2**."
        )
        
        st.markdown("---")
        
        # -----------------------------------------------------------------
        # BEISPIELANALYSE DER PC-WERTE
        # -----------------------------------------------------------------
        
        st.subheader("3. Interpretation der Koordinaten (PC-Werte)")
        st.markdown(
            "Wenn Sie √ºber einen Punkt (einen Tag) fahren, sehen Sie dessen exakte **Koordinaten** (PC1- und PC2-Werte). Diese Werte sind normiert, liegen also typischerweise zwischen ca. -0.1 und +0.1."
        )
        
        st.markdown("#### üìà PC1-Werte (Trend-Komponente)")
        st.markdown(
            "* **Hoher PC1-Wert (positiv, z.B. $> 0.05$):** Der Tag weist eine **stark bullische Struktur** auf (hohes Momentum, weit √ºber gleitenden Durchschnitten). Der Markt bewegte sich dominant nach oben.\n"
            "* **Niedriger PC1-Wert (negativ, z.B. $< -0.05$):** Der Tag weist eine **stark bearishe Struktur** auf. Der Markt befand sich in einem deutlichen Abw√§rtstrend.\n"
            "* **PC1 nahe Null:** Der Tag hatte eine **neutrale** (seitw√§rts gerichtete) Trendstruktur."
        )
        
        st.markdown("#### üå™Ô∏è PC2-Werte (Volatilit√§ts-Komponente)")
        st.markdown(
            "* **Hoher PC2-Wert (positiv, z.B. $> 0.04$):** Der Tag hatte eine **sehr hohe Volatilit√§t** (hohe Schwankungsbreite, gro√üe Kerzen). Dies deutet oft auf Phasen von **Angst oder Gier** hin.\n"
            "* **Niedriger PC2-Wert (negativ oder nahe Null):** Der Tag war **ruhig und stabil** (niedrige Volatilit√§t). Niedrige Werte bedeuten oft geringes Handelsinteresse oder eine Konsolidierungsphase."
        )
        
        st.markdown("---")
        st.markdown("#### üí° Konkretes Beispiel")
        st.markdown(
            "Nehmen wir an, der **Fokus-Tag** zeigt die Koordinaten **PC1 = 0.065** und **PC2 = 0.015**:\n"
            "1.  **PC1 (0.065):** Der Wert ist hoch und positiv. Interpretation: **Stark bullischer Trend** am Markt.\n"
            "2.  **PC2 (0.015):** Der Wert ist niedrig (nahe Null). Interpretation: **Niedrige bis moderate Volatilit√§t**.\n"
            "**Gesamt:** Der Tag befand sich in einem **starken, aber relativ ruhigen Aufw√§rtstrend** (Regime wahrscheinlich 'Bull')."
        )

# --- Tab 5: EXPLORATIVE ANALYSE (EDA) & THESEN ---
    with tab5_new:
        st.header("5. Explorative Analyse (EDA) & Thesen")
        st.info("Dieser Tab beleuchtet die **Datenbasis und Feature-Verteilung** und dient der empirischen **Pr√ºfung der Haupt-Thesen** aus der Marktanalyse.")
        



        # -----------------------------------------------------------------
        # 5.1 DATEN-√úBERBLICK & FEATURE-EXPLORATION (Die Rohdaten-Basis)
        # -----------------------------------------------------------------
        
        st.subheader("5.1 Daten-√úberblick: H√§ufigkeiten und Verteilungen")
        st.markdown("Der erste Schritt der Analyse ist die Pr√ºfung der Datenbasis und der Verteilung der wichtigsten Markt-Features.")
        
        st.markdown("---")
        
        st.markdown("### üìä Verteilung der t√§glichen Renditen")
        st.plotly_chart(create_return_distribution_plot(df_master), use_container_width=True)
        st.caption("Analyse: Die t√§gliche Rendite ist stark um 0 % zentriert, zeigt aber eine deutlich \
                linksschiefe Verteilung mit vielen Ausrei√üern. Die meisten Tage liegen im Bereich kleiner \
                Bewegungen (‚Äì2 % bis +2 %), w√§hrend seltene Extremereignisse deutlich weiter ausschlagen. \
                Diese heavy-tailed Struktur erkl√§rt die hohe Marktvolatilit√§t und bildet die Grundlage f√ºr \
                die sp√§teren Regime-Unterschiede. ‚Üí Die Renditen sind nicht normalverteilt, sondern stark \
                schwankungsanf√§llig."
                )


        st.markdown("### üîó Korrelationsmatrix der wichtigsten Features")
        st.plotly_chart(create_correlation_heatmap(df_master), use_container_width=True)
        st.caption("Analyse: Die Korrelationsmatrix zeigt, dass MA50 und MA200 extrem stark miteinander korrelieren \
                (r ‚âà 0.98), was typisch f√ºr langfristige Trendindikatoren ist. Die Tagesrenditen hingegen weisen \
                nahezu keine Korrelation zu anderen Features auf ‚Äì ein Hinweis auf die hohe Zuf√§lligkeit von \
                Tagesschwankungen. Volatilit√§t korreliert leicht negativ mit den gleitenden Durchschnitten, \
                was bedeutet: stabile Trends weisen geringere Volatilit√§t auf, w√§hrend chaotische Marktphasen \
                mit stark schwankenden Preisen einhergehen. ‚Üí Die Features erkl√§ren Marktphasen gut, aber die \
                Tagesrichtung nur sehr begrenzt."
                )

        st.markdown("### üìä H√§ufigkeit der Markt-Regime")
        st.plotly_chart(create_regime_frequency_bar_chart(df_master), use_container_width=True)

        st.caption(
            "Analyse: Der Markt ist klar von Aufw√§rtstrends gepr√§gt. Mit **‚âà 2.356 Bull-Tagen** dominiert das Bull-Regime "
            "den gr√∂√üten Teil der Historie. Das **Sideways-Regime tritt mit nur 380 Tagen** deutlich seltener auf, "
            "w√§hrend das **Bear-Regime mit 1.155 Tagen** zwar k√ºrzer ist, aber oft starke Abw√§rtsbewegungen enth√§lt. "
            "Diese asymmetrische Verteilung zeigt: Bitcoin verbringt die meiste Zeit im Aufw√§rtstrend, w√§hrend "
            "B√§renm√§rkte kompakt, aber wirtschaftlich besonders relevant sind. ‚Üí Diese Regimeverteilung bildet "
            "die Grundlage f√ºr die sp√§tere ML-Clusteranalyse."
)


        st.markdown("---")
        
        
        
        # -----------------------------------------------------------------
        # 5.2 THESENPR√úFUNG: BEWEISF√úHRUNG DURCH VISUALISIERUNG
        # -----------------------------------------------------------------
        st.subheader("5.2 Empirische Pr√ºfung der Zentralen Thesen")
        st.markdown("Die folgenden Visualisierungen dienen der direkten Pr√ºfung der wichtigsten Hypothesen zur Marktstruktur und Modellg√ºte. Der Erkl√§rungstext ist direkt unter dem jeweiligen Plot platziert.")
        st.markdown("---")

        # -----------------------------------------------------------------
        # THESEN BLOCK 1: These 1 (Regime-Unterschiede) ‚Äì Histogramm Version
        # -----------------------------------------------------------------
        st.markdown("#### üéØ Pr√ºfung These 1: Unterschiedliche Renditemuster in den Markt-Regimen")
        st.markdown("##### Histogramme: Verteilung der t√§glichen Renditen nach Regime")

        return_regime_hist = create_return_by_regime_histograms(df_master)
        st.plotly_chart(return_regime_hist, use_container_width=True)
        
        st.caption(
            "Analyse: Die Histogramme zeigen, dass Markt-Regime √∂konomisch real sind: "
            "Bear = √ºberwiegend negativ, Sideways = neutral um die Null-Linie, Bull = √ºberwiegend positiv. "
            "‚Üí These 1 best√§tigt ‚Äì die Regime unterscheiden sich signifikant in ihrer Renditeverteilung.")
        
        st.info(
            "**Beweisidee:** Diese drei Histogramme zeigen die Verteilung der t√§glichen Renditen getrennt nach den "
            "drei Markt-Regimen: Bear, Sideways und Bull.\n\n"
            "‚û°Ô∏è **Bear-Phasen:** Deutlich mehr negative Renditetage ‚Äì Schwerpunkt links im negativen Bereich.\n"
            "‚û°Ô∏è **Sideways-Phasen:** Enge Verteilung um die Null-Linie ‚Äì wenig Trend, geringe Dynamik.\n"
            "‚û°Ô∏è **Bull-Phasen:** Mehrheitlich positive Renditetage ‚Äì linke Seite schmal, rechte Seite ausgepr√§gt.\n\n"
            "‚û°Ô∏è **Interpretation:** Die Histogramme zeigen klar erkennbare Muster: B√§renm√§rkte verlieren statistisch Geld, "
            "Seitw√§rtsm√§rkte verlaufen stabil ohne gro√üe Ausschl√§ge, und Bullenm√§rkte generieren √ºberwiegend positive Renditen.\n"
            "‚û°Ô∏è **Fazit:** These 1 wird best√§tigt ‚Äì die drei Markt-Regime unterscheiden sich signifikant in der Form und "
            "Lage ihrer Renditeverteilung."
        )

        st.markdown("---")

        # -----------------------------------------------------------------
        # THESEN BLOCK 2: These 2 (Cluster-Phasen)
        # -----------------------------------------------------------------
        st.markdown("#### üéØ Pr√ºfung These 2: ML-Cluster bilden unterschiedliche Marktphasen ab")
        st.markdown("##### Violinplot: Renditeverteilung nach Cluster-ID")

        st.caption(
            "Cluster-Interpretation: üêÇ **Bull-Markt = Cluster 0**, ‚ûñ **Sideways = Cluster 1**, üêª **Bear-Markt = Cluster 2**")

        cluster_violin_fig = create_return_by_cluster_violinplot(df_master)
        st.plotly_chart(cluster_violin_fig, use_container_width=True)
        
        st.caption(
            "Analyse: Die Verteilungen zeigen klar unterschiedliche Marktphasen: "
            "Cluster 0 besitzt breite, positiv verschobene Renditen (Bull), "
            "Cluster 1 zeigt kompakte, neutrale Verteilungen (Sideways), "
            "und Cluster 2 besitzt linksschiefe, negative Renditen (Bear). "
            "‚Üí These 2 best√§tigt: ML-Cluster erkennen realwirtschaftliche Marktphasen.")


        st.info(
            "**Beweis:** Das ML-Clustering trennt den Markt in unterschiedliche Phasen, "
            "die klar unterschiedliche Renditemuster besitzen.\n\n"
            "‚û°Ô∏è Cluster mit hoher Volatilit√§t zeigen breitere Verteilungen.\n"
            "‚û°Ô∏è Trendphasen zeigen verschobene Renditezentren.\n"
            "‚û°Ô∏è Ruhige Marktphasen haben schmale, kompakte Verteilungen.\n\n"
            "‚û°Ô∏è **Fazit:** These 2 wird best√§tigt ‚Äì die ML-Cluster spiegeln realwirtschaftlich unterschiedliche Marktphasen wider."
        )

        st.markdown("---")

        # -----------------------------------------------------------------
        # THESEN BLOCK 3: These 3 (Hitrate pro Cluster)
        # -----------------------------------------------------------------
        st.markdown("#### üéØ Pr√ºfung These 3: Modellleistung unterscheidet sich zwischen den ML-Clustern")
        st.markdown("##### Balkendiagramm: Trefferquote pro Cluster")

        hitrate_cluster_fig = create_hitrate_by_cluster_bar_chart(df_master)
        st.plotly_chart(hitrate_cluster_fig, use_container_width=True)
                # ‚≠ê Analyse in kleiner Schrift direkt zum Chart
        st.caption(
            "Analyse: Die Trefferquote des Direction-Modells unterscheidet sich deutlich zwischen den ML-Clustern. "
            "Cluster 1 liefert die h√∂chste Trefferquote (~75%) und repr√§sentiert strukturierte Trendphasen, in denen Richtungsprognosen leicht fallen. "
            "Cluster 2 liegt im Mittelfeld (~65%). In Cluster 0 sinkt die Trefferquote auf nahezu Zufallsniveau (~52%) ‚Äì typisch f√ºr unklare Seitw√§rts- oder Umschwungphasen. "
            "‚Üí These 3 best√§tigt: Die Modellqualit√§t ist stark vom Marktumfeld abh√§ngig.")

        st.info(
            "**Beweisidee:** Die Trefferquote des Direction-Modells variiert je nach Marktphase.\n\n"
            "‚û°Ô∏è **Trendstarke Cluster (z. B. Bull)** erreichen h√∂here Trefferquoten, "
            "da Aufw√§rts- oder Abw√§rtsbewegungen klarer erkennbar sind.\n"
            "‚û°Ô∏è **Seitw√§rts-Cluster** liegen n√§her bei ~50% ‚Äì hier ist der Markt unstrukturiert, "
            "und die Richtung l√§sst sich schlechter prognostizieren.\n\n"
            "‚û°Ô∏è **Fazit:** These 3 wird best√§tigt ‚Äì die Modellleistung h√§ngt signifikant davon ab, "
            "in welchem ML-Cluster (und damit Marktumfeld) wir uns befinden."
        )

        # ‚≠ê Zusatzinfo: Zuordnung der Cluster zu Regimen (gleiche Logik wie bei These 2)
        st.caption(
            "Cluster-Interpretation: üêÇ **Bull-Markt = Cluster 0**, ‚ûñ **Sideways = Cluster 1**, üêª **Bear-Markt = Cluster 2**"
        )

        st.markdown("---")

        # # -----------------------------------------------------------------
        # # THESEN BLOCK 4: These 4 (Cluster vs. Regime Mapping)
        # # -----------------------------------------------------------------
        # st.markdown("#### üéØ Pr√ºfung These 4: ML-Cluster korrelieren mit den Markt-Regimen")
        # st.markdown("##### Heatmap: Wie gut mappt Cluster_3 auf Bear / Sideways / Bull?")

        # cluster_regime_fig = create_cluster_regime_heatmap(df_master)
        # st.plotly_chart(cluster_regime_fig, use_container_width=True)

        # st.info(
        #     "**Beweisidee:** Diese Heatmap zeigt, wie h√§ufig jeder ML-Cluster in einem bestimmten Markt-Regime "
        #     "(Bear, Sideways, Bull) vorkommt. Die dunkelsten Felder markieren dabei die h√∂chsten H√§ufigkeiten.\n\n"
        #     "‚û°Ô∏è **Starke Korrelation:** Wenn ein Cluster √ºberwiegend in einem einzigen Regime vorkommt, erkennt das ML-Modell "
        #     "die gleiche Struktur wie die klassischen Marktregime ‚Äì nur datengetriebener.\n"
        #     "‚û°Ô∏è **Feinere Granularit√§t:** ML-Cluster unterscheiden teilweise Unterphasen innerhalb eines Regimes "
        #     "(z. B. verschiedene Bull-Intensit√§ten oder unterschiedliche Seitw√§rtsstrukturen).\n\n"
        #     "‚û°Ô∏è **Fazit:** These 4 wird best√§tigt ‚Äì die ML-Cluster sind nicht nur zuf√§llige Gruppierungen, sondern bilden die "
        #     "Markt-Regime pr√§zise ab und liefern gleichzeitig zus√§tzliche Detailtiefe."
        # )

        # # üü¶ Zusatzinfo: Zuordnung der Cluster zu Marktregimen (sofern ermittelt)
        # st.caption(
        #     "Cluster-Interpretation (vereinfachte Zuordnung): "
        #     "üêª **Bear = Cluster 2**, ‚ûñ **Sideways = Cluster 1**, üêÇ **Bull = Cluster 0**. "
        #     "‚Üí ML-Cluster bilden die Regime weitgehend konsistent nach."
        # )

        # # üìä Analyse-Fu√ünote ‚Äì sehr kompakt & klein
        # st.caption(
        #     "Analyse: Die Heatmap zeigt eine klare diagonale Struktur ‚Äì jeder ML-Cluster konzentriert sich haupts√§chlich "
        #     "auf ein Regime. Das best√§tigt die statistische Abbildung der Marktphasen durch das Clustering. "
        #     "‚Üí ML-Cluster = datengetriebene Regime."
        # )

        # st.markdown("---")

        
        # -----------------------------------------------------------------
        # THESEN BLOCK 5: These 5 (Volatilit√§t vs. neg. Rendite)
        # -----------------------------------------------------------------
        st.markdown("#### üéØ Pr√ºfung These 5: Hohe Marktvolatilit√§t f√ºhrt statistisch zu schlechteren Renditen")
        st.markdown("##### Scatter Plot: Rendite vs. Volatilit√§t (30 Tage)")

        return_vol_fig = create_return_vs_volatility_scatter(df_master)
        st.plotly_chart(return_vol_fig, use_container_width=True)
        # Kleine kompakte Analyse (Caption)
        st.caption(
            "Analyse: Bei niedriger Volatilit√§t bleiben Renditen stabil um 0 %. Steigende Volatilit√§t f√ºhrt dazu, "
            "dass die Punktwolke sichtbar nach unten ausfranst. Besonders im Bear-Regime dominieren dann negative Ausschl√§ge."
        )

        st.info(
            "**Beweisidee:** Dieser Scatter Plot zeigt, wie sich t√§gliche Renditen (`Return`) unter unterschiedlichen "
            "Volatilit√§tsbedingungen (`Volatility30`) verhalten. Die graue Trendlinie (OLS) verdeutlicht den "
            "**durchschnittlichen statistischen Zusammenhang** zwischen beiden Gr√∂√üen.\n\n"

            "‚û°Ô∏è **Leicht fallende Trendlinie:** Die Linie neigt sich sichtbar nach unten. Das bedeutet: "
            "**Mit steigender Volatilit√§t sinken die durchschnittlichen Renditen.** Hohe Volatilit√§t ist also ein "
            "Fr√ºhindikator f√ºr Stress im Markt.\n\n"

            "‚û°Ô∏è **Grafikanalyse:** Die Punktwolke ist breit gestreut, aber man erkennt klare Muster:\n"
            "‚Ä¢ **Ab ca. 0.05 Volatilit√§t** verschieben sich viele Punkte deutlich in den negativen Renditebereich.\n"
            "‚Ä¢ **B√§renm√§rkte (rot)** sammeln sich rechts unten ‚Äì also bei hoher Volatilit√§t und gleichzeitig klar negativen Renditen.\n"
            "‚Ä¢ **Bullenm√§rkte (gr√ºn)** liegen h√§ufiger √ºber der Null-Linie, aber √ºberwiegend bei mittlerer Volatilit√§t ‚Äì "
            "Bull-M√§rkte funktionieren typischerweise besser in ruhigeren Phasen.\n"
            "‚Ä¢ **Seitw√§rtsm√§rkte (gelb)** konzentrieren sich eng um 0 % Rendite ‚Äì typisch f√ºr neutrale, "
            "weniger trendstarke Marktphasen.\n\n"

            "‚û°Ô∏è **Fazit:** These 5 wird best√§tigt ‚Äì **je chaotischer der Markt, desto h√§ufiger und tiefer sind Verluste**. "
            "Volatilit√§t wirkt als Risiko- und Stressbarometer des Marktes."
        )


        st.markdown("---")

        
        # -----------------------------------------------------------------
        # THESEN BLOCK 6: These 6 (MA-Differenz)
        # -----------------------------------------------------------------
        st.markdown("#### üéØ Pr√ºfung These 6: Gleitende Durchschnitte erkl√§ren Trendphasen klar und intuitiv")
        st.markdown("##### Boxplot: Unterschied zwischen kurzem und langem Durchschnitt (MA50 - MA200) nach Regime")

        ma_diff_fig = create_ma_diff_by_regime_boxplot(df_master) 
        st.plotly_chart(ma_diff_fig, use_container_width=True)

        # Kurze kompakte Analyse
        st.caption(
            "Analyse: In B√§renm√§rkten liegt der kurzfristige Durchschnitt stark unter dem langfristigen ‚Äì "
            "in Bullenm√§rkten deutlich dar√ºber. Seitw√§rtsphasen liegen dazwischen und schwanken breiter."
        )

        st.info(
            "**Beweisidee (einfach erkl√§rt):** Ein *gleitender Durchschnitt* ist nichts anderes als ein durchschnittlicher Preis "
            "√ºber einen bestimmten Zeitraum. Der **kurze Durchschnitt (50 Tage)** reagiert schneller, der **lange Durchschnitt (200 Tage)** "
            "reagiert langsamer.\n\n"
            
            "‚û°Ô∏è **Was bedeutet das?** Wenn der kurzfristige Durchschnitt √ºber dem langfristigen liegt, bewegt sich der Markt tendenziell "
            "aufw√§rts (Bull-Trend). Liegt er darunter, dominiert ein Abw√§rtstrend (Bear). Genau diese Logik nutzt man seit Jahrzehnten, "
            "um Trends zu erkennen.\n\n"
            
            "### üîç Was zeigt die Grafik?\n"
            "‚Ä¢ **Bear-Regime (rot):** Die Werte liegen klar **unter Null** ‚Äì der kurzfristige Preisverlauf liegt deutlich "
            "unter dem langfristigen. Das ist typisch f√ºr l√§ngere Abw√§rtsphasen.\n"
            "‚Ä¢ **Sideways-Regime (gelb):** Die Werte liegen **√ºber Null**, aber schwanken stark. Der Markt hat keine klare "
            "Richtung ‚Äì mal √ºber, mal unter dem langfristigen Trend.\n"
            "‚Ä¢ **Bull-Regime (gr√ºn):** Sehr enge Werte **√ºber Null** ‚Äì der kurzfristige Trend liegt stabil √ºber dem langfristigen. "
            "Der Markt steigt ruhig und relativ konstant.\n\n"
            
            "‚û°Ô∏è **Visuelle Interpretation:** Die graue Null-Linie ist die ‚ÄûTrendgrenze‚Äú. "
            "**B√§renm√§rkte liegen darunter**, **Bullenm√§rkte dar√ºber**, **Seitw√§rtsm√§rkte drumherum**.\n\n"

            "‚û°Ô∏è **Fazit:** These 6 wird klar best√§tigt ‚Äì der Unterschied zwischen kurzem und langem Durchschnitt ist ein "
            "intuitives und extrem zuverl√§ssiges Signal, um Marktphasen sauber zu unterscheiden."
        )

        st.markdown("---")


        # -----------------------------------------------------------------
        # THESEN BLOCK 7: These 7 (Direction Prediction ist extrem schwer)
        # -----------------------------------------------------------------
        st.markdown("#### üéØ Pr√ºfung These 7: ML kann Trends erkennen ‚Äì aber nicht zuverl√§ssig die Richtung des n√§chsten Tages")
        st.markdown("##### Balkendiagramm: Modellgenauigkeit (F1-Score) f√ºr Up/Down-Vorhersage")

        direction_fig = create_direction_model_performance_plot()
        st.plotly_chart(direction_fig, use_container_width=True)
        
        st.caption(
            "**Analyse:** Alle Modelle erreichen nur etwa **45‚Äì50 % F1-Score** und liegen damit "
            "**kaum √ºber dem Zufallsniveau**. Das zeigt: Selbst moderne ML-Algorithmen k√∂nnen die "
            "**Tagesrichtung (Up/Down)** des Bitcoin-Kurses **nicht zuverl√§ssig** vorhersagen. "
            "Kurzfristige Marktbewegungen sind √ºberwiegend **zuf√§llig**, stark **newsgetrieben** "
            "und schwer modellierbar. ‚Üí **Best√§tigung von These 7:** ML erkennt Marktstrukturen "
            "(Regime) sehr gut ‚Äì aber bei der Frage *‚ÄûSteigt der Kurs morgen?‚Äú* st√∂√üt es klar "
            "an seine Grenzen."
)


        st.info(
            "**Beweisidee:** Hier sehen wir die tats√§chliche Leistung verschiedener ML-Modelle bei der Vorhersage, "
            "ob der Bitcoin-Kurs am n√§chsten Tag **steigt (Up)** oder **f√§llt (Down)**.\n\n"

            "‚û°Ô∏è **F1-Scores liegen nur zwischen ~50‚Äì60%.** Das bedeutet: Die Modelle sind nur wenig besser als Zufall.\n\n"

            "‚û°Ô∏è **Warum ist das so?** Tagesbewegungen sind extrem zuf√§llig und werden von Nachrichten, Liquidit√§t, "
            "gro√üen Tradern und globalen Ereignissen dominiert. Diese Faktoren sieht das Modell nicht.\n\n"

            "‚û°Ô∏è **Wichtiger Lerneffekt:** ML kann **Marktphasen** hervorragend erkennen (siehe Regime), "
            "aber **Tagesrichtung** ist statistisch kaum vorhersagbar.\n\n"

            "‚û°Ô∏è **Fazit:** These 7 zeigt bewusst die Grenzen von Machine Learning: "
            "**Strukturen ja ‚Äî Hellsehen nein.**"
        )

        st.markdown("---")
        
                # -----------------------------------------------------------------
        # # 5.3 ROHDATEN-VORSCHAU
        # # -----------------------------------------------------------------
        # st.subheader("Rohdaten-Vorschau (Features)")
        # st.markdown("Anzeige der letzten Handelstage zur direkten explorativen √úberpr√ºfung von Features und dem zugewiesenen Markt-Regime.")
        
        # st.dataframe(df_master.tail(10).style.apply(highlight_focus_day, axis=1, focus_date_str=fokus_tag), use_container_width=True)

